{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we prepare the coco data set for generating cocept set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path= \"/ds/images/coco_2014/dataset_coco.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def get_coco_snipet(json_path, index=1, train=True):\n",
    "    \"\"\"\n",
    "    Extracts a single sample from a COCO-style JSON file and saves it as a new JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the COCO JSON file.\n",
    "        index (int): Index of the desired sample (1-based index).\n",
    "        train (bool): Whether to extract from the \"train\" or \"validation\" set.\n",
    "\n",
    "    Returns:\n",
    "        None: Saves a new JSON file with the snippet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the JSON file\n",
    "        with open(json_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        # Filter items from the \"images\" key based on the \"split\" field\n",
    "        subset = [item for item in coco_data.get(\"images\", []) if item.get(\"split\") == (\"train\" if train else \"val\")]\n",
    "\n",
    "        # Check if the index is within range\n",
    "        if index < 1 or index > len(subset):\n",
    "            raise IndexError(f\"Index {index} is out of range for the '{'train' if train else 'validation'}' dataset.\")\n",
    "\n",
    "        # Get the 1-based index sample\n",
    "        selected_sample = subset[index - 1]\n",
    "\n",
    "        # Create the output JSON structure\n",
    "        output_data = {\n",
    "            \"images\": [selected_sample],\n",
    "            \"info\": coco_data.get(\"info\", {}),\n",
    "            \"licenses\": coco_data.get(\"licenses\", []),\n",
    "            \"categories\": coco_data.get(\"categories\", []),\n",
    "        }\n",
    "\n",
    "        # Generate output file name\n",
    "        output_file = f\"coco_snippet_{'train' if train else 'validation'}_index_{index}.json\"\n",
    "\n",
    "        # Save the new JSON file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=4)\n",
    "\n",
    "        print(f\"Snippet saved to {output_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {json_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse JSON file: {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "# get_coco_snipet('path/to/coco.json', index=1, train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippet saved to coco_snippet_train_index_1.json\n"
     ]
    }
   ],
   "source": [
    "get_coco_snipet(dataset_path, index=1, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Function to split image into n patches\n",
    "def split_image(image, n):\n",
    "    width, height = image.size\n",
    "    patches = []\n",
    "    patch_width = width // n\n",
    "    patch_height = height // n\n",
    "    \n",
    "    # Create n x n patches\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            left = j * patch_width\n",
    "            upper = i * patch_height\n",
    "            right = left + patch_width\n",
    "            lower = upper + patch_height\n",
    "            \n",
    "            # Crop the image and add it to patches list\n",
    "            patch = image.crop((left, upper, right, lower))\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "# Function to save patches and update JSON\n",
    "def save_patches_and_update_json(json_data, root_dir, output_dir, n):\n",
    "    image_data = json_data['images'][0]\n",
    "    img_filepath = image_data['filepath']\n",
    "    img_filename = image_data['filename']\n",
    "    \n",
    "    # Load the image from the filepath\n",
    "    image_path = os.path.join(root_dir, img_filepath, img_filename)\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Split the image into n patches\n",
    "    patches = split_image(image, n)\n",
    "    \n",
    "\n",
    "    # Create a direcrtoy for patches\n",
    "    patch_root = img_filename.split('.')[0]\n",
    "    #patch_file_path = os.path.join(output_dir, patch_dir)\n",
    "    #os.makedirs(patch_file_path , exist_ok=True)\n",
    "    # Prepare new image JSON data for each patch\n",
    "    \n",
    "    new_images = []\n",
    "    for i, patch in enumerate(patches):\n",
    "        patch_filename = f\"{patch_root}_patch_{i+1}.jpg\"\n",
    "        patch_path = os.path.join(output_dir, patch_filename)\n",
    "        \n",
    "        # Save the patch image\n",
    "        patch.save(patch_path)\n",
    "        \n",
    "        # Create new JSON entry for the patch\n",
    "        new_image_data = image_data.copy()\n",
    "        new_image_data['filename'] = patch_filename\n",
    "        new_image_data['split'] = f\"{new_image_data['split']}_patch\"\n",
    "        new_image_data['filepath'] = os.path.split(output_dir)[-1]\n",
    "        new_image_data['imgid'] = f\"{image_data['imgid']}_{i+1}\"\n",
    "        new_image_data['sentences'] = [sent.copy() for sent in image_data['sentences']]  # Clone the sentences\n",
    "        \n",
    "        # Update the new image entry with updated sentid, filepath, and imgid\n",
    "        new_images.append(new_image_data)\n",
    "    \n",
    "    # Return the new images JSON data\n",
    "    json_data['images'] = new_images\n",
    "    return json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10x10 patches, saved them in /ds/images/xai_vision/train_patches, and updated the JSON file.\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "json_path = \"/home/kadir/xl-vlms/playground/coco_snippet_train_index_1.json\"\n",
    "with open(json_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Define your root and output directories\n",
    "root_dir = \"/ds/images/coco_2014\"\n",
    "output_dir = \"/ds/images/xai_vision/train_patches\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Number of patches (n)\n",
    "n = 10  # For example, split into 2x2 patches\n",
    "\n",
    "# Process the image and update JSON\n",
    "updated_json_data = save_patches_and_update_json(json_data, root_dir, output_dir, n)\n",
    "\n",
    "# Save the updated JSON data\n",
    "updated_json_path = \"/ds/images/xai_vision/patches_json_file.json\"\n",
    "with open(updated_json_path, 'w') as f:\n",
    "    json.dump(updated_json_data, f, indent=4)\n",
    "\n",
    "print(f\"Processed {n}x{n} patches, saved them in {output_dir}, and updated the JSON file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xl_vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
