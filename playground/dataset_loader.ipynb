{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_loader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.image_text_dataset import ImageTextDataset\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from datasets.constants import WORDS\n",
    "from models.constants import TASK_PROMPTS\n",
    "\n",
    "class COCODataset(ImageTextDataset):\n",
    "    def create_dataset(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        annotation_path = os.path.join(self.data_dir, self.annotation_file)\n",
    "        with open(annotation_path) as f:\n",
    "            karpathy_data = json.load(f)\n",
    "\n",
    "        data = []\n",
    "        for datum in karpathy_data[\"images\"]:\n",
    "            split_ = datum[\"split\"]\n",
    "            if split_ != self.split:\n",
    "                continue\n",
    "\n",
    "            img_id = datum[\"filename\"].split(\".\")[0]\n",
    "\n",
    "            if \"train\" in img_id:\n",
    "                source = \"train2014\"\n",
    "            elif \"val\" in img_id:\n",
    "                source = \"val2014\"\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Please specify the image directory for the image: {img_id}\"\n",
    "                )\n",
    "\n",
    "            image_path = os.path.join(self.data_dir, source, datum[\"filename\"])\n",
    "            instruction = TASK_PROMPTS.get(self.prompt_template, {}).get(\n",
    "                \"ShortCaptioning\", \"An image of \"\n",
    "            )\n",
    "            targets = [d[\"raw\"].strip() for d in datum[\"sentences\"]]\n",
    "            response = targets[0]  # take only the first caption\n",
    "\n",
    "            item = {\n",
    "                \"img_id\": img_id,\n",
    "                \"instruction\": instruction,\n",
    "                \"response\": response,\n",
    "                \"image\": image_path,\n",
    "                \"targets\": \"$$\".join(targets),\n",
    "            }\n",
    "            data.append(item)\n",
    "\n",
    "        if self.dataset_size > 0:\n",
    "            data = self.rng.choice(data, size=self.dataset_size, replace=False)\n",
    "\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cls = COCODataset\n",
    "dataset = dataset_cls(\n",
    "        data_dir='/ds/images/coco_2014',\n",
    "        annotation_file='/ds/images/coco_2014/dataset_coco.json',\n",
    "        questions_file='annotations.json',\n",
    "        split='train',\n",
    "        dataset_size=500,\n",
    "        seed=0,\n",
    "        dataset_name='coco',\n",
    "        mode=(\"val\" if True else \"train\"),\n",
    "        prompt_template='llava',\n",
    "        token_of_interest_num_samples=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_id': 'COCO_train2014_000000064070',\n",
       " 'instruction': '\\nProvide a one-sentence caption for the provided image.',\n",
       " 'response': 'The huge twin engine airliner has red, blue, and orange paint.',\n",
       " 'image': '/ds/images/coco_2014/train2014/COCO_train2014_000000064070.jpg',\n",
       " 'targets': 'The huge twin engine airliner has red, blue, and orange paint.$$Small air plane preparing to land over water.$$An very large airplane that has landed at an airport.$$A Southwest airplane taxis at an airport by the water.$$A Soutwest Airlines jet airplaine taxiing along a runway.',\n",
       " 'text': '\\nProvide a one-sentence caption for the provided image.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_id': 'COCO_train2014_000000064070', 'instruction': '\\nProvide a one-sentence caption for the provided image.', 'response': 'The huge twin engine airliner has red, blue, and orange paint.', 'image': '/ds/images/coco_2014/train2014/COCO_train2014_000000064070.jpg', 'targets': 'The huge twin engine airliner has red, blue, and orange paint.$$Small air plane preparing to land over water.$$An very large airplane that has landed at an airport.$$A Southwest airplane taxis at an airport by the water.$$A Soutwest Airlines jet airplaine taxiing along a runway.', 'text': '\\nProvide a one-sentence caption for the provided image.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "images\n"
     ]
    }
   ],
   "source": [
    "file = '/ds/images/FFA/physionet.org/files/ffa-ir-medical-report/1.0.0/ffair_annotation.json'\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Sort the keys alphabetically (or define your custom order)\n",
    "sorted_keys = sorted(data.keys())  # Replace this with your desired order if needed\n",
    "\n",
    "# Print the keys in the sorted order\n",
    "for key in sorted_keys:\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data[\"images\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def extract_relevant_sentences(text):\n",
    "    \"\"\"\n",
    "    Extract sections starting with INDICATION:, FINDINGS, and IMPRESSION.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "\n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"FINDINGS\"):\n",
    "            current_section = \"FINDINGS\"\n",
    "            sections[current_section] = line[len(\"FINDINGS:\"):].strip()\n",
    "        elif line.startswith(\"IMPRESSION\"):\n",
    "            current_section = \"IMPRESSION\"\n",
    "            sections[current_section] = line[len(\"IMPRESSION:\"):].strip()\n",
    "        elif current_section:\n",
    "            # Add to the current section if it spans multiple lines\n",
    "            sections[current_section] += \" \" + line\n",
    "\n",
    "    # Combine all relevant sections into a single string\n",
    "    relevant_text = \" \".join([sections[section] for section in [ \"FINDINGS\", \"IMPRESSION\"] if section in sections])\n",
    "    return relevant_text\n",
    "\n",
    "\n",
    "def organize_data_in_format(directory, output_file, train_ratio=0.8):\n",
    "    data = []\n",
    "    imgid = 0  # Unique ID for each image\n",
    "    \n",
    "    # Iterate through each entry in the directory\n",
    "    for entry in os.listdir(directory):\n",
    "        entry_path = os.path.join(directory, entry)\n",
    "        \n",
    "        # If the entry is a .txt file\n",
    "        if entry.endswith('.txt'):\n",
    "            # Read the content of the .txt file\n",
    "            with open(entry_path, 'r') as file:\n",
    "                raw_text = file.read()\n",
    "            \n",
    "            # Extract relevant sections from the text\n",
    "            relevant_text = extract_relevant_sentences(raw_text)\n",
    "            \n",
    "            if not relevant_text:\n",
    "                continue  # Skip if no relevant content is found\n",
    "            \n",
    "            # Tokenize the text into sentences\n",
    "            captions = relevant_text.split(\". \")  # Split by periods for sentences\n",
    "            \n",
    "            # Get folder name corresponding to the .txt file\n",
    "            folder_name = os.path.splitext(entry)[0]\n",
    "            folder_path = os.path.join(directory, folder_name)\n",
    "            \n",
    "            # Ensure the folder exists\n",
    "            if os.path.isdir(folder_path):\n",
    "                for img_file in os.listdir(folder_path):\n",
    "                    if img_file.endswith('.dcm'):\n",
    "                        # Full path to the image\n",
    "                        image_path = os.path.join(folder_path, img_file)\n",
    "                        \n",
    "                        # Extract a filename for the image\n",
    "                        filename = os.path.basename(image_path)\n",
    "                        \n",
    "                        # Create a new entry for the image\n",
    "                        entry_data = {\n",
    "                            'filepath': folder_name,\n",
    "                            'sentids': list(range(imgid * 100, imgid * 100 + len(captions))),\n",
    "                            'filename': filename,\n",
    "                            'imgid': imgid,\n",
    "                            'split': 'val',  # Temporary, will adjust after splitting\n",
    "                            'sentences': [],\n",
    "                            'cocoid': imgid\n",
    "                        }\n",
    "                        \n",
    "                        # Add sentences to the entry\n",
    "                        for i, caption in enumerate(captions):\n",
    "                            tokens = caption.strip().split()  # Tokenize caption\n",
    "                            entry_data['sentences'].append({\n",
    "                                'tokens': tokens,\n",
    "                                'raw': caption.strip(),\n",
    "                                'imgid': imgid,\n",
    "                                'sentid': imgid * 100 + i\n",
    "                            })\n",
    "                        \n",
    "                        # Increment the image ID for each image\n",
    "                        imgid += 1\n",
    "                        \n",
    "                        # Add the entry to the data\n",
    "                        data.append(entry_data)\n",
    "    \n",
    "    # Split data into train and val sets\n",
    "    train_data, val_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "    \n",
    "    # Update the split key for train and val datasets\n",
    "    for item in train_data:\n",
    "        item['split'] = 'train'\n",
    "    for item in val_data:\n",
    "        item['split'] = 'val'\n",
    "    \n",
    "    # Wrap the data under the \"images\" key and add \"type\" as \"coco\"\n",
    "    output_data = {\n",
    "        \"images\": train_data + val_data,\n",
    "        \"type\": \"coco\"\n",
    "    }\n",
    "    \n",
    "    # Save the result to a JSON file\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(output_data, json_file, indent=2)\n",
    "    \n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to organized_data.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "directory = \"/netscratch/kadir/xl-vlms/sample_data\"  # Replace with the path to your data directory\n",
    "output_file = \"organized_data.json\"\n",
    "organize_data_in_format(directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.image_text_dataset import ImageTextDataset\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from datasets.constants import WORDS\n",
    "from models.constants import TASK_PROMPTS\n",
    "\n",
    "class XRAYdataset(ImageTextDataset):\n",
    "    def create_dataset(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        annotation_path = os.path.join(self.data_dir, self.annotation_file)\n",
    "        with open(annotation_path) as f:\n",
    "            karpathy_data = json.load(f)\n",
    "\n",
    "        data = []\n",
    "        for datum in karpathy_data[\"images\"]:\n",
    "            split_ = datum[\"split\"]\n",
    "            if split_ != self.split:\n",
    "                continue\n",
    "\n",
    "            img_id = datum[\"filename\"].split(\".\")[0]\n",
    "\n",
    "\n",
    "            image_path = os.path.join(self.data_dir, datum[\"filepath\"], datum[\"filename\"])\n",
    "            instruction = TASK_PROMPTS.get(self.prompt_template, {}).get(\n",
    "                 \"Findings\", \"Please provide a detailed finding of chest X-ray \"\n",
    "            )\n",
    "            targets = [d[\"raw\"].strip() for d in datum[\"sentences\"]]\n",
    "            response = targets[0]  # take only the first caption\n",
    "\n",
    "            item = {\n",
    "                \"img_id\": img_id,\n",
    "                \"instruction\": instruction,\n",
    "                \"response\": response,\n",
    "                \"image\": image_path,\n",
    "                \"targets\": \"$$\".join(targets),\n",
    "            }\n",
    "            data.append(item)\n",
    "\n",
    "        if self.dataset_size > 0:\n",
    "            data = self.rng.choice(data, size=self.dataset_size, replace=False)\n",
    "\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets.image_text_dataset import ImageTextDataset\n",
    "\n",
    "dataset_cls = XRAYdataset\n",
    "dataset = dataset_cls(\n",
    "        data_dir='/netscratch/kadir/xl-vlms/sample_data',\n",
    "        annotation_file='/home/kadir/xl-vlms/playground/organized_data.json',\n",
    "        questions_file='annotations.json',\n",
    "        split='train',\n",
    "        dataset_size=10,\n",
    "        seed=0,\n",
    "        dataset_name='coco',\n",
    "        mode=(\"val\" if True else \"train\"),\n",
    "        prompt_template='chextagent',\n",
    "        token_of_interest_num_samples=-1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/netscratch/kadir/xl-vlms/sample_data/s56105641/b35e1481-bc791f9a-dfafdae3-ef03967d-3c1cff7c.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s57473907/e31b309f-72d6b511-383b9a4b-7d20b6a2-de6dfa53.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s51820245/a7d4ea5c-3d1aa223-f8df852b-e4e86c60-a0b2036f.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s58896631/ad9066c8-3e02858c-2e0556ed-aaded1ee-50a2d7fe.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s50423320/0f50bb73-76e0eff4-0f30b9ef-02dc8eeb-58a4c2ec.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s59888039/6d4403f0-08e832f3-5478e509-3050a2d3-b81b2140.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s59910851/5dcfd4b2-d8d01a9e-8e0cdb88-59b1e9a4-8b86cc80.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s56105641/348151d9-e2d62d79-e214f7e6-27bcf16b-cdc41317.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s58672627/e84af51e-f53d38fb-3ab64326-73b0188f-4071f521.dcm\n",
      "/netscratch/kadir/xl-vlms/sample_data/s51232822/b62a4d54-81fc729c-89ae4a70-347c7b1b-058d49a1.dcm\n",
      "All file paths exist!\n"
     ]
    }
   ],
   "source": [
    "dataset[0]\n",
    "missing_files = []\n",
    "for entry in dataset:\n",
    "        full_path = entry.get(\"image\", '')\n",
    "        print(full_path)\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(full_path):\n",
    "            missing_files.append(full_path)\n",
    "    \n",
    "# Report results\n",
    "if missing_files:\n",
    "    print(\"The following files are missing:\")\n",
    "    for missing in missing_files:\n",
    "        print(missing)\n",
    "else:\n",
    "    print(\"All file paths exist!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xl_vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
