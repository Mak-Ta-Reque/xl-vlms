{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/mnt/abka03/xl_vlm/cache'\n",
    "cache_dir = '/mnt/abka03/xl_vlm/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 1: Setup constant\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/StanfordAIMI/CheXagent-8b:\n",
      "- processing_chexagent.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/StanfordAIMI/CheXagent-8b:\n",
      "- configuration_chexagent.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/StanfordAIMI/CheXagent-8b:\n",
      "- modeling_chexagent.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards:  57%|█████▋    | 4/7 [08:15<06:11, 123.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStanfordAIMI/CheXagent-8b\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir \u001b[38;5;241m=\u001b[39mcache_dir,  trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStanfordAIMI/CheXagent-8b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStanfordAIMI/CheXagent-8b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/transformers/modeling_utils.py:3990\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3987\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3989\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3990\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3999\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4001\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4002\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4003\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4006\u001b[0m     is_safetensors_available()\n\u001b[1;32m   4007\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   4008\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4009\u001b[0m ):\n\u001b[1;32m   4010\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/transformers/utils/hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    454\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.conda/envs/xl_vlm/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "processor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", cache_dir =cache_dir,  trust_remote_code=True)\n",
    "generation_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"StanfordAIMI/CheXagent-8b\", cache_dir = cache_dir, torch_dtype=dtype, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchinfo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchinfo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary(model, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(model, depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vision_model\n",
      "vision_model.embeddings\n",
      "vision_model.embeddings.patch_embedding\n",
      "vision_model.encoder\n",
      "vision_model.encoder.layers\n",
      "vision_model.encoder.layers.0\n",
      "vision_model.encoder.layers.0.self_attn\n",
      "vision_model.encoder.layers.0.self_attn.dropout\n",
      "vision_model.encoder.layers.0.self_attn.qkv\n",
      "vision_model.encoder.layers.0.self_attn.projection\n",
      "vision_model.encoder.layers.0.layer_norm1\n",
      "vision_model.encoder.layers.0.mlp\n",
      "vision_model.encoder.layers.0.mlp.activation_fn\n",
      "vision_model.encoder.layers.0.mlp.fc1\n",
      "vision_model.encoder.layers.0.mlp.fc2\n",
      "vision_model.encoder.layers.0.layer_norm2\n",
      "vision_model.encoder.layers.1\n",
      "vision_model.encoder.layers.1.self_attn\n",
      "vision_model.encoder.layers.1.self_attn.dropout\n",
      "vision_model.encoder.layers.1.self_attn.qkv\n",
      "vision_model.encoder.layers.1.self_attn.projection\n",
      "vision_model.encoder.layers.1.layer_norm1\n",
      "vision_model.encoder.layers.1.mlp\n",
      "vision_model.encoder.layers.1.mlp.activation_fn\n",
      "vision_model.encoder.layers.1.mlp.fc1\n",
      "vision_model.encoder.layers.1.mlp.fc2\n",
      "vision_model.encoder.layers.1.layer_norm2\n",
      "vision_model.encoder.layers.2\n",
      "vision_model.encoder.layers.2.self_attn\n",
      "vision_model.encoder.layers.2.self_attn.dropout\n",
      "vision_model.encoder.layers.2.self_attn.qkv\n",
      "vision_model.encoder.layers.2.self_attn.projection\n",
      "vision_model.encoder.layers.2.layer_norm1\n",
      "vision_model.encoder.layers.2.mlp\n",
      "vision_model.encoder.layers.2.mlp.activation_fn\n",
      "vision_model.encoder.layers.2.mlp.fc1\n",
      "vision_model.encoder.layers.2.mlp.fc2\n",
      "vision_model.encoder.layers.2.layer_norm2\n",
      "vision_model.encoder.layers.3\n",
      "vision_model.encoder.layers.3.self_attn\n",
      "vision_model.encoder.layers.3.self_attn.dropout\n",
      "vision_model.encoder.layers.3.self_attn.qkv\n",
      "vision_model.encoder.layers.3.self_attn.projection\n",
      "vision_model.encoder.layers.3.layer_norm1\n",
      "vision_model.encoder.layers.3.mlp\n",
      "vision_model.encoder.layers.3.mlp.activation_fn\n",
      "vision_model.encoder.layers.3.mlp.fc1\n",
      "vision_model.encoder.layers.3.mlp.fc2\n",
      "vision_model.encoder.layers.3.layer_norm2\n",
      "vision_model.encoder.layers.4\n",
      "vision_model.encoder.layers.4.self_attn\n",
      "vision_model.encoder.layers.4.self_attn.dropout\n",
      "vision_model.encoder.layers.4.self_attn.qkv\n",
      "vision_model.encoder.layers.4.self_attn.projection\n",
      "vision_model.encoder.layers.4.layer_norm1\n",
      "vision_model.encoder.layers.4.mlp\n",
      "vision_model.encoder.layers.4.mlp.activation_fn\n",
      "vision_model.encoder.layers.4.mlp.fc1\n",
      "vision_model.encoder.layers.4.mlp.fc2\n",
      "vision_model.encoder.layers.4.layer_norm2\n",
      "vision_model.encoder.layers.5\n",
      "vision_model.encoder.layers.5.self_attn\n",
      "vision_model.encoder.layers.5.self_attn.dropout\n",
      "vision_model.encoder.layers.5.self_attn.qkv\n",
      "vision_model.encoder.layers.5.self_attn.projection\n",
      "vision_model.encoder.layers.5.layer_norm1\n",
      "vision_model.encoder.layers.5.mlp\n",
      "vision_model.encoder.layers.5.mlp.activation_fn\n",
      "vision_model.encoder.layers.5.mlp.fc1\n",
      "vision_model.encoder.layers.5.mlp.fc2\n",
      "vision_model.encoder.layers.5.layer_norm2\n",
      "vision_model.encoder.layers.6\n",
      "vision_model.encoder.layers.6.self_attn\n",
      "vision_model.encoder.layers.6.self_attn.dropout\n",
      "vision_model.encoder.layers.6.self_attn.qkv\n",
      "vision_model.encoder.layers.6.self_attn.projection\n",
      "vision_model.encoder.layers.6.layer_norm1\n",
      "vision_model.encoder.layers.6.mlp\n",
      "vision_model.encoder.layers.6.mlp.activation_fn\n",
      "vision_model.encoder.layers.6.mlp.fc1\n",
      "vision_model.encoder.layers.6.mlp.fc2\n",
      "vision_model.encoder.layers.6.layer_norm2\n",
      "vision_model.encoder.layers.7\n",
      "vision_model.encoder.layers.7.self_attn\n",
      "vision_model.encoder.layers.7.self_attn.dropout\n",
      "vision_model.encoder.layers.7.self_attn.qkv\n",
      "vision_model.encoder.layers.7.self_attn.projection\n",
      "vision_model.encoder.layers.7.layer_norm1\n",
      "vision_model.encoder.layers.7.mlp\n",
      "vision_model.encoder.layers.7.mlp.activation_fn\n",
      "vision_model.encoder.layers.7.mlp.fc1\n",
      "vision_model.encoder.layers.7.mlp.fc2\n",
      "vision_model.encoder.layers.7.layer_norm2\n",
      "vision_model.encoder.layers.8\n",
      "vision_model.encoder.layers.8.self_attn\n",
      "vision_model.encoder.layers.8.self_attn.dropout\n",
      "vision_model.encoder.layers.8.self_attn.qkv\n",
      "vision_model.encoder.layers.8.self_attn.projection\n",
      "vision_model.encoder.layers.8.layer_norm1\n",
      "vision_model.encoder.layers.8.mlp\n",
      "vision_model.encoder.layers.8.mlp.activation_fn\n",
      "vision_model.encoder.layers.8.mlp.fc1\n",
      "vision_model.encoder.layers.8.mlp.fc2\n",
      "vision_model.encoder.layers.8.layer_norm2\n",
      "vision_model.encoder.layers.9\n",
      "vision_model.encoder.layers.9.self_attn\n",
      "vision_model.encoder.layers.9.self_attn.dropout\n",
      "vision_model.encoder.layers.9.self_attn.qkv\n",
      "vision_model.encoder.layers.9.self_attn.projection\n",
      "vision_model.encoder.layers.9.layer_norm1\n",
      "vision_model.encoder.layers.9.mlp\n",
      "vision_model.encoder.layers.9.mlp.activation_fn\n",
      "vision_model.encoder.layers.9.mlp.fc1\n",
      "vision_model.encoder.layers.9.mlp.fc2\n",
      "vision_model.encoder.layers.9.layer_norm2\n",
      "vision_model.encoder.layers.10\n",
      "vision_model.encoder.layers.10.self_attn\n",
      "vision_model.encoder.layers.10.self_attn.dropout\n",
      "vision_model.encoder.layers.10.self_attn.qkv\n",
      "vision_model.encoder.layers.10.self_attn.projection\n",
      "vision_model.encoder.layers.10.layer_norm1\n",
      "vision_model.encoder.layers.10.mlp\n",
      "vision_model.encoder.layers.10.mlp.activation_fn\n",
      "vision_model.encoder.layers.10.mlp.fc1\n",
      "vision_model.encoder.layers.10.mlp.fc2\n",
      "vision_model.encoder.layers.10.layer_norm2\n",
      "vision_model.encoder.layers.11\n",
      "vision_model.encoder.layers.11.self_attn\n",
      "vision_model.encoder.layers.11.self_attn.dropout\n",
      "vision_model.encoder.layers.11.self_attn.qkv\n",
      "vision_model.encoder.layers.11.self_attn.projection\n",
      "vision_model.encoder.layers.11.layer_norm1\n",
      "vision_model.encoder.layers.11.mlp\n",
      "vision_model.encoder.layers.11.mlp.activation_fn\n",
      "vision_model.encoder.layers.11.mlp.fc1\n",
      "vision_model.encoder.layers.11.mlp.fc2\n",
      "vision_model.encoder.layers.11.layer_norm2\n",
      "vision_model.encoder.layers.12\n",
      "vision_model.encoder.layers.12.self_attn\n",
      "vision_model.encoder.layers.12.self_attn.dropout\n",
      "vision_model.encoder.layers.12.self_attn.qkv\n",
      "vision_model.encoder.layers.12.self_attn.projection\n",
      "vision_model.encoder.layers.12.layer_norm1\n",
      "vision_model.encoder.layers.12.mlp\n",
      "vision_model.encoder.layers.12.mlp.activation_fn\n",
      "vision_model.encoder.layers.12.mlp.fc1\n",
      "vision_model.encoder.layers.12.mlp.fc2\n",
      "vision_model.encoder.layers.12.layer_norm2\n",
      "vision_model.encoder.layers.13\n",
      "vision_model.encoder.layers.13.self_attn\n",
      "vision_model.encoder.layers.13.self_attn.dropout\n",
      "vision_model.encoder.layers.13.self_attn.qkv\n",
      "vision_model.encoder.layers.13.self_attn.projection\n",
      "vision_model.encoder.layers.13.layer_norm1\n",
      "vision_model.encoder.layers.13.mlp\n",
      "vision_model.encoder.layers.13.mlp.activation_fn\n",
      "vision_model.encoder.layers.13.mlp.fc1\n",
      "vision_model.encoder.layers.13.mlp.fc2\n",
      "vision_model.encoder.layers.13.layer_norm2\n",
      "vision_model.encoder.layers.14\n",
      "vision_model.encoder.layers.14.self_attn\n",
      "vision_model.encoder.layers.14.self_attn.dropout\n",
      "vision_model.encoder.layers.14.self_attn.qkv\n",
      "vision_model.encoder.layers.14.self_attn.projection\n",
      "vision_model.encoder.layers.14.layer_norm1\n",
      "vision_model.encoder.layers.14.mlp\n",
      "vision_model.encoder.layers.14.mlp.activation_fn\n",
      "vision_model.encoder.layers.14.mlp.fc1\n",
      "vision_model.encoder.layers.14.mlp.fc2\n",
      "vision_model.encoder.layers.14.layer_norm2\n",
      "vision_model.encoder.layers.15\n",
      "vision_model.encoder.layers.15.self_attn\n",
      "vision_model.encoder.layers.15.self_attn.dropout\n",
      "vision_model.encoder.layers.15.self_attn.qkv\n",
      "vision_model.encoder.layers.15.self_attn.projection\n",
      "vision_model.encoder.layers.15.layer_norm1\n",
      "vision_model.encoder.layers.15.mlp\n",
      "vision_model.encoder.layers.15.mlp.activation_fn\n",
      "vision_model.encoder.layers.15.mlp.fc1\n",
      "vision_model.encoder.layers.15.mlp.fc2\n",
      "vision_model.encoder.layers.15.layer_norm2\n",
      "vision_model.encoder.layers.16\n",
      "vision_model.encoder.layers.16.self_attn\n",
      "vision_model.encoder.layers.16.self_attn.dropout\n",
      "vision_model.encoder.layers.16.self_attn.qkv\n",
      "vision_model.encoder.layers.16.self_attn.projection\n",
      "vision_model.encoder.layers.16.layer_norm1\n",
      "vision_model.encoder.layers.16.mlp\n",
      "vision_model.encoder.layers.16.mlp.activation_fn\n",
      "vision_model.encoder.layers.16.mlp.fc1\n",
      "vision_model.encoder.layers.16.mlp.fc2\n",
      "vision_model.encoder.layers.16.layer_norm2\n",
      "vision_model.encoder.layers.17\n",
      "vision_model.encoder.layers.17.self_attn\n",
      "vision_model.encoder.layers.17.self_attn.dropout\n",
      "vision_model.encoder.layers.17.self_attn.qkv\n",
      "vision_model.encoder.layers.17.self_attn.projection\n",
      "vision_model.encoder.layers.17.layer_norm1\n",
      "vision_model.encoder.layers.17.mlp\n",
      "vision_model.encoder.layers.17.mlp.activation_fn\n",
      "vision_model.encoder.layers.17.mlp.fc1\n",
      "vision_model.encoder.layers.17.mlp.fc2\n",
      "vision_model.encoder.layers.17.layer_norm2\n",
      "vision_model.encoder.layers.18\n",
      "vision_model.encoder.layers.18.self_attn\n",
      "vision_model.encoder.layers.18.self_attn.dropout\n",
      "vision_model.encoder.layers.18.self_attn.qkv\n",
      "vision_model.encoder.layers.18.self_attn.projection\n",
      "vision_model.encoder.layers.18.layer_norm1\n",
      "vision_model.encoder.layers.18.mlp\n",
      "vision_model.encoder.layers.18.mlp.activation_fn\n",
      "vision_model.encoder.layers.18.mlp.fc1\n",
      "vision_model.encoder.layers.18.mlp.fc2\n",
      "vision_model.encoder.layers.18.layer_norm2\n",
      "vision_model.encoder.layers.19\n",
      "vision_model.encoder.layers.19.self_attn\n",
      "vision_model.encoder.layers.19.self_attn.dropout\n",
      "vision_model.encoder.layers.19.self_attn.qkv\n",
      "vision_model.encoder.layers.19.self_attn.projection\n",
      "vision_model.encoder.layers.19.layer_norm1\n",
      "vision_model.encoder.layers.19.mlp\n",
      "vision_model.encoder.layers.19.mlp.activation_fn\n",
      "vision_model.encoder.layers.19.mlp.fc1\n",
      "vision_model.encoder.layers.19.mlp.fc2\n",
      "vision_model.encoder.layers.19.layer_norm2\n",
      "vision_model.encoder.layers.20\n",
      "vision_model.encoder.layers.20.self_attn\n",
      "vision_model.encoder.layers.20.self_attn.dropout\n",
      "vision_model.encoder.layers.20.self_attn.qkv\n",
      "vision_model.encoder.layers.20.self_attn.projection\n",
      "vision_model.encoder.layers.20.layer_norm1\n",
      "vision_model.encoder.layers.20.mlp\n",
      "vision_model.encoder.layers.20.mlp.activation_fn\n",
      "vision_model.encoder.layers.20.mlp.fc1\n",
      "vision_model.encoder.layers.20.mlp.fc2\n",
      "vision_model.encoder.layers.20.layer_norm2\n",
      "vision_model.encoder.layers.21\n",
      "vision_model.encoder.layers.21.self_attn\n",
      "vision_model.encoder.layers.21.self_attn.dropout\n",
      "vision_model.encoder.layers.21.self_attn.qkv\n",
      "vision_model.encoder.layers.21.self_attn.projection\n",
      "vision_model.encoder.layers.21.layer_norm1\n",
      "vision_model.encoder.layers.21.mlp\n",
      "vision_model.encoder.layers.21.mlp.activation_fn\n",
      "vision_model.encoder.layers.21.mlp.fc1\n",
      "vision_model.encoder.layers.21.mlp.fc2\n",
      "vision_model.encoder.layers.21.layer_norm2\n",
      "vision_model.encoder.layers.22\n",
      "vision_model.encoder.layers.22.self_attn\n",
      "vision_model.encoder.layers.22.self_attn.dropout\n",
      "vision_model.encoder.layers.22.self_attn.qkv\n",
      "vision_model.encoder.layers.22.self_attn.projection\n",
      "vision_model.encoder.layers.22.layer_norm1\n",
      "vision_model.encoder.layers.22.mlp\n",
      "vision_model.encoder.layers.22.mlp.activation_fn\n",
      "vision_model.encoder.layers.22.mlp.fc1\n",
      "vision_model.encoder.layers.22.mlp.fc2\n",
      "vision_model.encoder.layers.22.layer_norm2\n",
      "vision_model.encoder.layers.23\n",
      "vision_model.encoder.layers.23.self_attn\n",
      "vision_model.encoder.layers.23.self_attn.dropout\n",
      "vision_model.encoder.layers.23.self_attn.qkv\n",
      "vision_model.encoder.layers.23.self_attn.projection\n",
      "vision_model.encoder.layers.23.layer_norm1\n",
      "vision_model.encoder.layers.23.mlp\n",
      "vision_model.encoder.layers.23.mlp.activation_fn\n",
      "vision_model.encoder.layers.23.mlp.fc1\n",
      "vision_model.encoder.layers.23.mlp.fc2\n",
      "vision_model.encoder.layers.23.layer_norm2\n",
      "vision_model.encoder.layers.24\n",
      "vision_model.encoder.layers.24.self_attn\n",
      "vision_model.encoder.layers.24.self_attn.dropout\n",
      "vision_model.encoder.layers.24.self_attn.qkv\n",
      "vision_model.encoder.layers.24.self_attn.projection\n",
      "vision_model.encoder.layers.24.layer_norm1\n",
      "vision_model.encoder.layers.24.mlp\n",
      "vision_model.encoder.layers.24.mlp.activation_fn\n",
      "vision_model.encoder.layers.24.mlp.fc1\n",
      "vision_model.encoder.layers.24.mlp.fc2\n",
      "vision_model.encoder.layers.24.layer_norm2\n",
      "vision_model.encoder.layers.25\n",
      "vision_model.encoder.layers.25.self_attn\n",
      "vision_model.encoder.layers.25.self_attn.dropout\n",
      "vision_model.encoder.layers.25.self_attn.qkv\n",
      "vision_model.encoder.layers.25.self_attn.projection\n",
      "vision_model.encoder.layers.25.layer_norm1\n",
      "vision_model.encoder.layers.25.mlp\n",
      "vision_model.encoder.layers.25.mlp.activation_fn\n",
      "vision_model.encoder.layers.25.mlp.fc1\n",
      "vision_model.encoder.layers.25.mlp.fc2\n",
      "vision_model.encoder.layers.25.layer_norm2\n",
      "vision_model.encoder.layers.26\n",
      "vision_model.encoder.layers.26.self_attn\n",
      "vision_model.encoder.layers.26.self_attn.dropout\n",
      "vision_model.encoder.layers.26.self_attn.qkv\n",
      "vision_model.encoder.layers.26.self_attn.projection\n",
      "vision_model.encoder.layers.26.layer_norm1\n",
      "vision_model.encoder.layers.26.mlp\n",
      "vision_model.encoder.layers.26.mlp.activation_fn\n",
      "vision_model.encoder.layers.26.mlp.fc1\n",
      "vision_model.encoder.layers.26.mlp.fc2\n",
      "vision_model.encoder.layers.26.layer_norm2\n",
      "vision_model.encoder.layers.27\n",
      "vision_model.encoder.layers.27.self_attn\n",
      "vision_model.encoder.layers.27.self_attn.dropout\n",
      "vision_model.encoder.layers.27.self_attn.qkv\n",
      "vision_model.encoder.layers.27.self_attn.projection\n",
      "vision_model.encoder.layers.27.layer_norm1\n",
      "vision_model.encoder.layers.27.mlp\n",
      "vision_model.encoder.layers.27.mlp.activation_fn\n",
      "vision_model.encoder.layers.27.mlp.fc1\n",
      "vision_model.encoder.layers.27.mlp.fc2\n",
      "vision_model.encoder.layers.27.layer_norm2\n",
      "vision_model.encoder.layers.28\n",
      "vision_model.encoder.layers.28.self_attn\n",
      "vision_model.encoder.layers.28.self_attn.dropout\n",
      "vision_model.encoder.layers.28.self_attn.qkv\n",
      "vision_model.encoder.layers.28.self_attn.projection\n",
      "vision_model.encoder.layers.28.layer_norm1\n",
      "vision_model.encoder.layers.28.mlp\n",
      "vision_model.encoder.layers.28.mlp.activation_fn\n",
      "vision_model.encoder.layers.28.mlp.fc1\n",
      "vision_model.encoder.layers.28.mlp.fc2\n",
      "vision_model.encoder.layers.28.layer_norm2\n",
      "vision_model.encoder.layers.29\n",
      "vision_model.encoder.layers.29.self_attn\n",
      "vision_model.encoder.layers.29.self_attn.dropout\n",
      "vision_model.encoder.layers.29.self_attn.qkv\n",
      "vision_model.encoder.layers.29.self_attn.projection\n",
      "vision_model.encoder.layers.29.layer_norm1\n",
      "vision_model.encoder.layers.29.mlp\n",
      "vision_model.encoder.layers.29.mlp.activation_fn\n",
      "vision_model.encoder.layers.29.mlp.fc1\n",
      "vision_model.encoder.layers.29.mlp.fc2\n",
      "vision_model.encoder.layers.29.layer_norm2\n",
      "vision_model.encoder.layers.30\n",
      "vision_model.encoder.layers.30.self_attn\n",
      "vision_model.encoder.layers.30.self_attn.dropout\n",
      "vision_model.encoder.layers.30.self_attn.qkv\n",
      "vision_model.encoder.layers.30.self_attn.projection\n",
      "vision_model.encoder.layers.30.layer_norm1\n",
      "vision_model.encoder.layers.30.mlp\n",
      "vision_model.encoder.layers.30.mlp.activation_fn\n",
      "vision_model.encoder.layers.30.mlp.fc1\n",
      "vision_model.encoder.layers.30.mlp.fc2\n",
      "vision_model.encoder.layers.30.layer_norm2\n",
      "vision_model.encoder.layers.31\n",
      "vision_model.encoder.layers.31.self_attn\n",
      "vision_model.encoder.layers.31.self_attn.dropout\n",
      "vision_model.encoder.layers.31.self_attn.qkv\n",
      "vision_model.encoder.layers.31.self_attn.projection\n",
      "vision_model.encoder.layers.31.layer_norm1\n",
      "vision_model.encoder.layers.31.mlp\n",
      "vision_model.encoder.layers.31.mlp.activation_fn\n",
      "vision_model.encoder.layers.31.mlp.fc1\n",
      "vision_model.encoder.layers.31.mlp.fc2\n",
      "vision_model.encoder.layers.31.layer_norm2\n",
      "vision_model.encoder.layers.32\n",
      "vision_model.encoder.layers.32.self_attn\n",
      "vision_model.encoder.layers.32.self_attn.dropout\n",
      "vision_model.encoder.layers.32.self_attn.qkv\n",
      "vision_model.encoder.layers.32.self_attn.projection\n",
      "vision_model.encoder.layers.32.layer_norm1\n",
      "vision_model.encoder.layers.32.mlp\n",
      "vision_model.encoder.layers.32.mlp.activation_fn\n",
      "vision_model.encoder.layers.32.mlp.fc1\n",
      "vision_model.encoder.layers.32.mlp.fc2\n",
      "vision_model.encoder.layers.32.layer_norm2\n",
      "vision_model.encoder.layers.33\n",
      "vision_model.encoder.layers.33.self_attn\n",
      "vision_model.encoder.layers.33.self_attn.dropout\n",
      "vision_model.encoder.layers.33.self_attn.qkv\n",
      "vision_model.encoder.layers.33.self_attn.projection\n",
      "vision_model.encoder.layers.33.layer_norm1\n",
      "vision_model.encoder.layers.33.mlp\n",
      "vision_model.encoder.layers.33.mlp.activation_fn\n",
      "vision_model.encoder.layers.33.mlp.fc1\n",
      "vision_model.encoder.layers.33.mlp.fc2\n",
      "vision_model.encoder.layers.33.layer_norm2\n",
      "vision_model.encoder.layers.34\n",
      "vision_model.encoder.layers.34.self_attn\n",
      "vision_model.encoder.layers.34.self_attn.dropout\n",
      "vision_model.encoder.layers.34.self_attn.qkv\n",
      "vision_model.encoder.layers.34.self_attn.projection\n",
      "vision_model.encoder.layers.34.layer_norm1\n",
      "vision_model.encoder.layers.34.mlp\n",
      "vision_model.encoder.layers.34.mlp.activation_fn\n",
      "vision_model.encoder.layers.34.mlp.fc1\n",
      "vision_model.encoder.layers.34.mlp.fc2\n",
      "vision_model.encoder.layers.34.layer_norm2\n",
      "vision_model.encoder.layers.35\n",
      "vision_model.encoder.layers.35.self_attn\n",
      "vision_model.encoder.layers.35.self_attn.dropout\n",
      "vision_model.encoder.layers.35.self_attn.qkv\n",
      "vision_model.encoder.layers.35.self_attn.projection\n",
      "vision_model.encoder.layers.35.layer_norm1\n",
      "vision_model.encoder.layers.35.mlp\n",
      "vision_model.encoder.layers.35.mlp.activation_fn\n",
      "vision_model.encoder.layers.35.mlp.fc1\n",
      "vision_model.encoder.layers.35.mlp.fc2\n",
      "vision_model.encoder.layers.35.layer_norm2\n",
      "vision_model.encoder.layers.36\n",
      "vision_model.encoder.layers.36.self_attn\n",
      "vision_model.encoder.layers.36.self_attn.dropout\n",
      "vision_model.encoder.layers.36.self_attn.qkv\n",
      "vision_model.encoder.layers.36.self_attn.projection\n",
      "vision_model.encoder.layers.36.layer_norm1\n",
      "vision_model.encoder.layers.36.mlp\n",
      "vision_model.encoder.layers.36.mlp.activation_fn\n",
      "vision_model.encoder.layers.36.mlp.fc1\n",
      "vision_model.encoder.layers.36.mlp.fc2\n",
      "vision_model.encoder.layers.36.layer_norm2\n",
      "vision_model.encoder.layers.37\n",
      "vision_model.encoder.layers.37.self_attn\n",
      "vision_model.encoder.layers.37.self_attn.dropout\n",
      "vision_model.encoder.layers.37.self_attn.qkv\n",
      "vision_model.encoder.layers.37.self_attn.projection\n",
      "vision_model.encoder.layers.37.layer_norm1\n",
      "vision_model.encoder.layers.37.mlp\n",
      "vision_model.encoder.layers.37.mlp.activation_fn\n",
      "vision_model.encoder.layers.37.mlp.fc1\n",
      "vision_model.encoder.layers.37.mlp.fc2\n",
      "vision_model.encoder.layers.37.layer_norm2\n",
      "vision_model.encoder.layers.38\n",
      "vision_model.encoder.layers.38.self_attn\n",
      "vision_model.encoder.layers.38.self_attn.dropout\n",
      "vision_model.encoder.layers.38.self_attn.qkv\n",
      "vision_model.encoder.layers.38.self_attn.projection\n",
      "vision_model.encoder.layers.38.layer_norm1\n",
      "vision_model.encoder.layers.38.mlp\n",
      "vision_model.encoder.layers.38.mlp.activation_fn\n",
      "vision_model.encoder.layers.38.mlp.fc1\n",
      "vision_model.encoder.layers.38.mlp.fc2\n",
      "vision_model.encoder.layers.38.layer_norm2\n",
      "vision_model.encoder.layers.39\n",
      "vision_model.encoder.layers.39.self_attn\n",
      "vision_model.encoder.layers.39.self_attn.dropout\n",
      "vision_model.encoder.layers.39.self_attn.qkv\n",
      "vision_model.encoder.layers.39.self_attn.projection\n",
      "vision_model.encoder.layers.39.layer_norm1\n",
      "vision_model.encoder.layers.39.mlp\n",
      "vision_model.encoder.layers.39.mlp.activation_fn\n",
      "vision_model.encoder.layers.39.mlp.fc1\n",
      "vision_model.encoder.layers.39.mlp.fc2\n",
      "vision_model.encoder.layers.39.layer_norm2\n",
      "vision_model.post_layernorm\n",
      "qformer\n",
      "qformer.layernorm\n",
      "qformer.dropout\n",
      "qformer.encoder\n",
      "qformer.encoder.layer\n",
      "qformer.encoder.layer.0\n",
      "qformer.encoder.layer.0.attention\n",
      "qformer.encoder.layer.0.attention.attention\n",
      "qformer.encoder.layer.0.attention.attention.query\n",
      "qformer.encoder.layer.0.attention.attention.key\n",
      "qformer.encoder.layer.0.attention.attention.value\n",
      "qformer.encoder.layer.0.attention.attention.dropout\n",
      "qformer.encoder.layer.0.attention.output\n",
      "qformer.encoder.layer.0.attention.output.dense\n",
      "qformer.encoder.layer.0.attention.output.LayerNorm\n",
      "qformer.encoder.layer.0.attention.output.dropout\n",
      "qformer.encoder.layer.0.crossattention\n",
      "qformer.encoder.layer.0.crossattention.attention\n",
      "qformer.encoder.layer.0.crossattention.attention.query\n",
      "qformer.encoder.layer.0.crossattention.attention.key\n",
      "qformer.encoder.layer.0.crossattention.attention.value\n",
      "qformer.encoder.layer.0.crossattention.attention.dropout\n",
      "qformer.encoder.layer.0.crossattention.output\n",
      "qformer.encoder.layer.0.crossattention.output.dense\n",
      "qformer.encoder.layer.0.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.0.crossattention.output.dropout\n",
      "qformer.encoder.layer.0.intermediate_query\n",
      "qformer.encoder.layer.0.intermediate_query.dense\n",
      "qformer.encoder.layer.0.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.0.output_query\n",
      "qformer.encoder.layer.0.output_query.dense\n",
      "qformer.encoder.layer.0.output_query.LayerNorm\n",
      "qformer.encoder.layer.0.output_query.dropout\n",
      "qformer.encoder.layer.1\n",
      "qformer.encoder.layer.1.attention\n",
      "qformer.encoder.layer.1.attention.attention\n",
      "qformer.encoder.layer.1.attention.attention.query\n",
      "qformer.encoder.layer.1.attention.attention.key\n",
      "qformer.encoder.layer.1.attention.attention.value\n",
      "qformer.encoder.layer.1.attention.attention.dropout\n",
      "qformer.encoder.layer.1.attention.output\n",
      "qformer.encoder.layer.1.attention.output.dense\n",
      "qformer.encoder.layer.1.attention.output.LayerNorm\n",
      "qformer.encoder.layer.1.attention.output.dropout\n",
      "qformer.encoder.layer.1.intermediate_query\n",
      "qformer.encoder.layer.1.intermediate_query.dense\n",
      "qformer.encoder.layer.1.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.1.output_query\n",
      "qformer.encoder.layer.1.output_query.dense\n",
      "qformer.encoder.layer.1.output_query.LayerNorm\n",
      "qformer.encoder.layer.1.output_query.dropout\n",
      "qformer.encoder.layer.2\n",
      "qformer.encoder.layer.2.attention\n",
      "qformer.encoder.layer.2.attention.attention\n",
      "qformer.encoder.layer.2.attention.attention.query\n",
      "qformer.encoder.layer.2.attention.attention.key\n",
      "qformer.encoder.layer.2.attention.attention.value\n",
      "qformer.encoder.layer.2.attention.attention.dropout\n",
      "qformer.encoder.layer.2.attention.output\n",
      "qformer.encoder.layer.2.attention.output.dense\n",
      "qformer.encoder.layer.2.attention.output.LayerNorm\n",
      "qformer.encoder.layer.2.attention.output.dropout\n",
      "qformer.encoder.layer.2.crossattention\n",
      "qformer.encoder.layer.2.crossattention.attention\n",
      "qformer.encoder.layer.2.crossattention.attention.query\n",
      "qformer.encoder.layer.2.crossattention.attention.key\n",
      "qformer.encoder.layer.2.crossattention.attention.value\n",
      "qformer.encoder.layer.2.crossattention.attention.dropout\n",
      "qformer.encoder.layer.2.crossattention.output\n",
      "qformer.encoder.layer.2.crossattention.output.dense\n",
      "qformer.encoder.layer.2.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.2.crossattention.output.dropout\n",
      "qformer.encoder.layer.2.intermediate_query\n",
      "qformer.encoder.layer.2.intermediate_query.dense\n",
      "qformer.encoder.layer.2.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.2.output_query\n",
      "qformer.encoder.layer.2.output_query.dense\n",
      "qformer.encoder.layer.2.output_query.LayerNorm\n",
      "qformer.encoder.layer.2.output_query.dropout\n",
      "qformer.encoder.layer.3\n",
      "qformer.encoder.layer.3.attention\n",
      "qformer.encoder.layer.3.attention.attention\n",
      "qformer.encoder.layer.3.attention.attention.query\n",
      "qformer.encoder.layer.3.attention.attention.key\n",
      "qformer.encoder.layer.3.attention.attention.value\n",
      "qformer.encoder.layer.3.attention.attention.dropout\n",
      "qformer.encoder.layer.3.attention.output\n",
      "qformer.encoder.layer.3.attention.output.dense\n",
      "qformer.encoder.layer.3.attention.output.LayerNorm\n",
      "qformer.encoder.layer.3.attention.output.dropout\n",
      "qformer.encoder.layer.3.intermediate_query\n",
      "qformer.encoder.layer.3.intermediate_query.dense\n",
      "qformer.encoder.layer.3.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.3.output_query\n",
      "qformer.encoder.layer.3.output_query.dense\n",
      "qformer.encoder.layer.3.output_query.LayerNorm\n",
      "qformer.encoder.layer.3.output_query.dropout\n",
      "qformer.encoder.layer.4\n",
      "qformer.encoder.layer.4.attention\n",
      "qformer.encoder.layer.4.attention.attention\n",
      "qformer.encoder.layer.4.attention.attention.query\n",
      "qformer.encoder.layer.4.attention.attention.key\n",
      "qformer.encoder.layer.4.attention.attention.value\n",
      "qformer.encoder.layer.4.attention.attention.dropout\n",
      "qformer.encoder.layer.4.attention.output\n",
      "qformer.encoder.layer.4.attention.output.dense\n",
      "qformer.encoder.layer.4.attention.output.LayerNorm\n",
      "qformer.encoder.layer.4.attention.output.dropout\n",
      "qformer.encoder.layer.4.crossattention\n",
      "qformer.encoder.layer.4.crossattention.attention\n",
      "qformer.encoder.layer.4.crossattention.attention.query\n",
      "qformer.encoder.layer.4.crossattention.attention.key\n",
      "qformer.encoder.layer.4.crossattention.attention.value\n",
      "qformer.encoder.layer.4.crossattention.attention.dropout\n",
      "qformer.encoder.layer.4.crossattention.output\n",
      "qformer.encoder.layer.4.crossattention.output.dense\n",
      "qformer.encoder.layer.4.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.4.crossattention.output.dropout\n",
      "qformer.encoder.layer.4.intermediate_query\n",
      "qformer.encoder.layer.4.intermediate_query.dense\n",
      "qformer.encoder.layer.4.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.4.output_query\n",
      "qformer.encoder.layer.4.output_query.dense\n",
      "qformer.encoder.layer.4.output_query.LayerNorm\n",
      "qformer.encoder.layer.4.output_query.dropout\n",
      "qformer.encoder.layer.5\n",
      "qformer.encoder.layer.5.attention\n",
      "qformer.encoder.layer.5.attention.attention\n",
      "qformer.encoder.layer.5.attention.attention.query\n",
      "qformer.encoder.layer.5.attention.attention.key\n",
      "qformer.encoder.layer.5.attention.attention.value\n",
      "qformer.encoder.layer.5.attention.attention.dropout\n",
      "qformer.encoder.layer.5.attention.output\n",
      "qformer.encoder.layer.5.attention.output.dense\n",
      "qformer.encoder.layer.5.attention.output.LayerNorm\n",
      "qformer.encoder.layer.5.attention.output.dropout\n",
      "qformer.encoder.layer.5.intermediate_query\n",
      "qformer.encoder.layer.5.intermediate_query.dense\n",
      "qformer.encoder.layer.5.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.5.output_query\n",
      "qformer.encoder.layer.5.output_query.dense\n",
      "qformer.encoder.layer.5.output_query.LayerNorm\n",
      "qformer.encoder.layer.5.output_query.dropout\n",
      "qformer.encoder.layer.6\n",
      "qformer.encoder.layer.6.attention\n",
      "qformer.encoder.layer.6.attention.attention\n",
      "qformer.encoder.layer.6.attention.attention.query\n",
      "qformer.encoder.layer.6.attention.attention.key\n",
      "qformer.encoder.layer.6.attention.attention.value\n",
      "qformer.encoder.layer.6.attention.attention.dropout\n",
      "qformer.encoder.layer.6.attention.output\n",
      "qformer.encoder.layer.6.attention.output.dense\n",
      "qformer.encoder.layer.6.attention.output.LayerNorm\n",
      "qformer.encoder.layer.6.attention.output.dropout\n",
      "qformer.encoder.layer.6.crossattention\n",
      "qformer.encoder.layer.6.crossattention.attention\n",
      "qformer.encoder.layer.6.crossattention.attention.query\n",
      "qformer.encoder.layer.6.crossattention.attention.key\n",
      "qformer.encoder.layer.6.crossattention.attention.value\n",
      "qformer.encoder.layer.6.crossattention.attention.dropout\n",
      "qformer.encoder.layer.6.crossattention.output\n",
      "qformer.encoder.layer.6.crossattention.output.dense\n",
      "qformer.encoder.layer.6.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.6.crossattention.output.dropout\n",
      "qformer.encoder.layer.6.intermediate_query\n",
      "qformer.encoder.layer.6.intermediate_query.dense\n",
      "qformer.encoder.layer.6.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.6.output_query\n",
      "qformer.encoder.layer.6.output_query.dense\n",
      "qformer.encoder.layer.6.output_query.LayerNorm\n",
      "qformer.encoder.layer.6.output_query.dropout\n",
      "qformer.encoder.layer.7\n",
      "qformer.encoder.layer.7.attention\n",
      "qformer.encoder.layer.7.attention.attention\n",
      "qformer.encoder.layer.7.attention.attention.query\n",
      "qformer.encoder.layer.7.attention.attention.key\n",
      "qformer.encoder.layer.7.attention.attention.value\n",
      "qformer.encoder.layer.7.attention.attention.dropout\n",
      "qformer.encoder.layer.7.attention.output\n",
      "qformer.encoder.layer.7.attention.output.dense\n",
      "qformer.encoder.layer.7.attention.output.LayerNorm\n",
      "qformer.encoder.layer.7.attention.output.dropout\n",
      "qformer.encoder.layer.7.intermediate_query\n",
      "qformer.encoder.layer.7.intermediate_query.dense\n",
      "qformer.encoder.layer.7.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.7.output_query\n",
      "qformer.encoder.layer.7.output_query.dense\n",
      "qformer.encoder.layer.7.output_query.LayerNorm\n",
      "qformer.encoder.layer.7.output_query.dropout\n",
      "qformer.encoder.layer.8\n",
      "qformer.encoder.layer.8.attention\n",
      "qformer.encoder.layer.8.attention.attention\n",
      "qformer.encoder.layer.8.attention.attention.query\n",
      "qformer.encoder.layer.8.attention.attention.key\n",
      "qformer.encoder.layer.8.attention.attention.value\n",
      "qformer.encoder.layer.8.attention.attention.dropout\n",
      "qformer.encoder.layer.8.attention.output\n",
      "qformer.encoder.layer.8.attention.output.dense\n",
      "qformer.encoder.layer.8.attention.output.LayerNorm\n",
      "qformer.encoder.layer.8.attention.output.dropout\n",
      "qformer.encoder.layer.8.crossattention\n",
      "qformer.encoder.layer.8.crossattention.attention\n",
      "qformer.encoder.layer.8.crossattention.attention.query\n",
      "qformer.encoder.layer.8.crossattention.attention.key\n",
      "qformer.encoder.layer.8.crossattention.attention.value\n",
      "qformer.encoder.layer.8.crossattention.attention.dropout\n",
      "qformer.encoder.layer.8.crossattention.output\n",
      "qformer.encoder.layer.8.crossattention.output.dense\n",
      "qformer.encoder.layer.8.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.8.crossattention.output.dropout\n",
      "qformer.encoder.layer.8.intermediate_query\n",
      "qformer.encoder.layer.8.intermediate_query.dense\n",
      "qformer.encoder.layer.8.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.8.output_query\n",
      "qformer.encoder.layer.8.output_query.dense\n",
      "qformer.encoder.layer.8.output_query.LayerNorm\n",
      "qformer.encoder.layer.8.output_query.dropout\n",
      "qformer.encoder.layer.9\n",
      "qformer.encoder.layer.9.attention\n",
      "qformer.encoder.layer.9.attention.attention\n",
      "qformer.encoder.layer.9.attention.attention.query\n",
      "qformer.encoder.layer.9.attention.attention.key\n",
      "qformer.encoder.layer.9.attention.attention.value\n",
      "qformer.encoder.layer.9.attention.attention.dropout\n",
      "qformer.encoder.layer.9.attention.output\n",
      "qformer.encoder.layer.9.attention.output.dense\n",
      "qformer.encoder.layer.9.attention.output.LayerNorm\n",
      "qformer.encoder.layer.9.attention.output.dropout\n",
      "qformer.encoder.layer.9.intermediate_query\n",
      "qformer.encoder.layer.9.intermediate_query.dense\n",
      "qformer.encoder.layer.9.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.9.output_query\n",
      "qformer.encoder.layer.9.output_query.dense\n",
      "qformer.encoder.layer.9.output_query.LayerNorm\n",
      "qformer.encoder.layer.9.output_query.dropout\n",
      "qformer.encoder.layer.10\n",
      "qformer.encoder.layer.10.attention\n",
      "qformer.encoder.layer.10.attention.attention\n",
      "qformer.encoder.layer.10.attention.attention.query\n",
      "qformer.encoder.layer.10.attention.attention.key\n",
      "qformer.encoder.layer.10.attention.attention.value\n",
      "qformer.encoder.layer.10.attention.attention.dropout\n",
      "qformer.encoder.layer.10.attention.output\n",
      "qformer.encoder.layer.10.attention.output.dense\n",
      "qformer.encoder.layer.10.attention.output.LayerNorm\n",
      "qformer.encoder.layer.10.attention.output.dropout\n",
      "qformer.encoder.layer.10.crossattention\n",
      "qformer.encoder.layer.10.crossattention.attention\n",
      "qformer.encoder.layer.10.crossattention.attention.query\n",
      "qformer.encoder.layer.10.crossattention.attention.key\n",
      "qformer.encoder.layer.10.crossattention.attention.value\n",
      "qformer.encoder.layer.10.crossattention.attention.dropout\n",
      "qformer.encoder.layer.10.crossattention.output\n",
      "qformer.encoder.layer.10.crossattention.output.dense\n",
      "qformer.encoder.layer.10.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.10.crossattention.output.dropout\n",
      "qformer.encoder.layer.10.intermediate_query\n",
      "qformer.encoder.layer.10.intermediate_query.dense\n",
      "qformer.encoder.layer.10.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.10.output_query\n",
      "qformer.encoder.layer.10.output_query.dense\n",
      "qformer.encoder.layer.10.output_query.LayerNorm\n",
      "qformer.encoder.layer.10.output_query.dropout\n",
      "qformer.encoder.layer.11\n",
      "qformer.encoder.layer.11.attention\n",
      "qformer.encoder.layer.11.attention.attention\n",
      "qformer.encoder.layer.11.attention.attention.query\n",
      "qformer.encoder.layer.11.attention.attention.key\n",
      "qformer.encoder.layer.11.attention.attention.value\n",
      "qformer.encoder.layer.11.attention.attention.dropout\n",
      "qformer.encoder.layer.11.attention.output\n",
      "qformer.encoder.layer.11.attention.output.dense\n",
      "qformer.encoder.layer.11.attention.output.LayerNorm\n",
      "qformer.encoder.layer.11.attention.output.dropout\n",
      "qformer.encoder.layer.11.intermediate_query\n",
      "qformer.encoder.layer.11.intermediate_query.dense\n",
      "qformer.encoder.layer.11.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.11.output_query\n",
      "qformer.encoder.layer.11.output_query.dense\n",
      "qformer.encoder.layer.11.output_query.LayerNorm\n",
      "qformer.encoder.layer.11.output_query.dropout\n",
      "language_projection\n",
      "language_model\n",
      "language_model.model\n",
      "language_model.model.embed_tokens\n",
      "language_model.model.layers\n",
      "language_model.model.layers.0\n",
      "language_model.model.layers.0.self_attn\n",
      "language_model.model.layers.0.self_attn.q_proj\n",
      "language_model.model.layers.0.self_attn.k_proj\n",
      "language_model.model.layers.0.self_attn.v_proj\n",
      "language_model.model.layers.0.self_attn.o_proj\n",
      "language_model.model.layers.0.self_attn.rotary_emb\n",
      "language_model.model.layers.0.mlp\n",
      "language_model.model.layers.0.mlp.gate_proj\n",
      "language_model.model.layers.0.mlp.up_proj\n",
      "language_model.model.layers.0.mlp.down_proj\n",
      "language_model.model.layers.0.mlp.act_fn\n",
      "language_model.model.layers.0.input_layernorm\n",
      "language_model.model.layers.0.post_attention_layernorm\n",
      "language_model.model.layers.1\n",
      "language_model.model.layers.1.self_attn\n",
      "language_model.model.layers.1.self_attn.q_proj\n",
      "language_model.model.layers.1.self_attn.k_proj\n",
      "language_model.model.layers.1.self_attn.v_proj\n",
      "language_model.model.layers.1.self_attn.o_proj\n",
      "language_model.model.layers.1.self_attn.rotary_emb\n",
      "language_model.model.layers.1.mlp\n",
      "language_model.model.layers.1.mlp.gate_proj\n",
      "language_model.model.layers.1.mlp.up_proj\n",
      "language_model.model.layers.1.mlp.down_proj\n",
      "language_model.model.layers.1.mlp.act_fn\n",
      "language_model.model.layers.1.input_layernorm\n",
      "language_model.model.layers.1.post_attention_layernorm\n",
      "language_model.model.layers.2\n",
      "language_model.model.layers.2.self_attn\n",
      "language_model.model.layers.2.self_attn.q_proj\n",
      "language_model.model.layers.2.self_attn.k_proj\n",
      "language_model.model.layers.2.self_attn.v_proj\n",
      "language_model.model.layers.2.self_attn.o_proj\n",
      "language_model.model.layers.2.self_attn.rotary_emb\n",
      "language_model.model.layers.2.mlp\n",
      "language_model.model.layers.2.mlp.gate_proj\n",
      "language_model.model.layers.2.mlp.up_proj\n",
      "language_model.model.layers.2.mlp.down_proj\n",
      "language_model.model.layers.2.mlp.act_fn\n",
      "language_model.model.layers.2.input_layernorm\n",
      "language_model.model.layers.2.post_attention_layernorm\n",
      "language_model.model.layers.3\n",
      "language_model.model.layers.3.self_attn\n",
      "language_model.model.layers.3.self_attn.q_proj\n",
      "language_model.model.layers.3.self_attn.k_proj\n",
      "language_model.model.layers.3.self_attn.v_proj\n",
      "language_model.model.layers.3.self_attn.o_proj\n",
      "language_model.model.layers.3.self_attn.rotary_emb\n",
      "language_model.model.layers.3.mlp\n",
      "language_model.model.layers.3.mlp.gate_proj\n",
      "language_model.model.layers.3.mlp.up_proj\n",
      "language_model.model.layers.3.mlp.down_proj\n",
      "language_model.model.layers.3.mlp.act_fn\n",
      "language_model.model.layers.3.input_layernorm\n",
      "language_model.model.layers.3.post_attention_layernorm\n",
      "language_model.model.layers.4\n",
      "language_model.model.layers.4.self_attn\n",
      "language_model.model.layers.4.self_attn.q_proj\n",
      "language_model.model.layers.4.self_attn.k_proj\n",
      "language_model.model.layers.4.self_attn.v_proj\n",
      "language_model.model.layers.4.self_attn.o_proj\n",
      "language_model.model.layers.4.self_attn.rotary_emb\n",
      "language_model.model.layers.4.mlp\n",
      "language_model.model.layers.4.mlp.gate_proj\n",
      "language_model.model.layers.4.mlp.up_proj\n",
      "language_model.model.layers.4.mlp.down_proj\n",
      "language_model.model.layers.4.mlp.act_fn\n",
      "language_model.model.layers.4.input_layernorm\n",
      "language_model.model.layers.4.post_attention_layernorm\n",
      "language_model.model.layers.5\n",
      "language_model.model.layers.5.self_attn\n",
      "language_model.model.layers.5.self_attn.q_proj\n",
      "language_model.model.layers.5.self_attn.k_proj\n",
      "language_model.model.layers.5.self_attn.v_proj\n",
      "language_model.model.layers.5.self_attn.o_proj\n",
      "language_model.model.layers.5.self_attn.rotary_emb\n",
      "language_model.model.layers.5.mlp\n",
      "language_model.model.layers.5.mlp.gate_proj\n",
      "language_model.model.layers.5.mlp.up_proj\n",
      "language_model.model.layers.5.mlp.down_proj\n",
      "language_model.model.layers.5.mlp.act_fn\n",
      "language_model.model.layers.5.input_layernorm\n",
      "language_model.model.layers.5.post_attention_layernorm\n",
      "language_model.model.layers.6\n",
      "language_model.model.layers.6.self_attn\n",
      "language_model.model.layers.6.self_attn.q_proj\n",
      "language_model.model.layers.6.self_attn.k_proj\n",
      "language_model.model.layers.6.self_attn.v_proj\n",
      "language_model.model.layers.6.self_attn.o_proj\n",
      "language_model.model.layers.6.self_attn.rotary_emb\n",
      "language_model.model.layers.6.mlp\n",
      "language_model.model.layers.6.mlp.gate_proj\n",
      "language_model.model.layers.6.mlp.up_proj\n",
      "language_model.model.layers.6.mlp.down_proj\n",
      "language_model.model.layers.6.mlp.act_fn\n",
      "language_model.model.layers.6.input_layernorm\n",
      "language_model.model.layers.6.post_attention_layernorm\n",
      "language_model.model.layers.7\n",
      "language_model.model.layers.7.self_attn\n",
      "language_model.model.layers.7.self_attn.q_proj\n",
      "language_model.model.layers.7.self_attn.k_proj\n",
      "language_model.model.layers.7.self_attn.v_proj\n",
      "language_model.model.layers.7.self_attn.o_proj\n",
      "language_model.model.layers.7.self_attn.rotary_emb\n",
      "language_model.model.layers.7.mlp\n",
      "language_model.model.layers.7.mlp.gate_proj\n",
      "language_model.model.layers.7.mlp.up_proj\n",
      "language_model.model.layers.7.mlp.down_proj\n",
      "language_model.model.layers.7.mlp.act_fn\n",
      "language_model.model.layers.7.input_layernorm\n",
      "language_model.model.layers.7.post_attention_layernorm\n",
      "language_model.model.layers.8\n",
      "language_model.model.layers.8.self_attn\n",
      "language_model.model.layers.8.self_attn.q_proj\n",
      "language_model.model.layers.8.self_attn.k_proj\n",
      "language_model.model.layers.8.self_attn.v_proj\n",
      "language_model.model.layers.8.self_attn.o_proj\n",
      "language_model.model.layers.8.self_attn.rotary_emb\n",
      "language_model.model.layers.8.mlp\n",
      "language_model.model.layers.8.mlp.gate_proj\n",
      "language_model.model.layers.8.mlp.up_proj\n",
      "language_model.model.layers.8.mlp.down_proj\n",
      "language_model.model.layers.8.mlp.act_fn\n",
      "language_model.model.layers.8.input_layernorm\n",
      "language_model.model.layers.8.post_attention_layernorm\n",
      "language_model.model.layers.9\n",
      "language_model.model.layers.9.self_attn\n",
      "language_model.model.layers.9.self_attn.q_proj\n",
      "language_model.model.layers.9.self_attn.k_proj\n",
      "language_model.model.layers.9.self_attn.v_proj\n",
      "language_model.model.layers.9.self_attn.o_proj\n",
      "language_model.model.layers.9.self_attn.rotary_emb\n",
      "language_model.model.layers.9.mlp\n",
      "language_model.model.layers.9.mlp.gate_proj\n",
      "language_model.model.layers.9.mlp.up_proj\n",
      "language_model.model.layers.9.mlp.down_proj\n",
      "language_model.model.layers.9.mlp.act_fn\n",
      "language_model.model.layers.9.input_layernorm\n",
      "language_model.model.layers.9.post_attention_layernorm\n",
      "language_model.model.layers.10\n",
      "language_model.model.layers.10.self_attn\n",
      "language_model.model.layers.10.self_attn.q_proj\n",
      "language_model.model.layers.10.self_attn.k_proj\n",
      "language_model.model.layers.10.self_attn.v_proj\n",
      "language_model.model.layers.10.self_attn.o_proj\n",
      "language_model.model.layers.10.self_attn.rotary_emb\n",
      "language_model.model.layers.10.mlp\n",
      "language_model.model.layers.10.mlp.gate_proj\n",
      "language_model.model.layers.10.mlp.up_proj\n",
      "language_model.model.layers.10.mlp.down_proj\n",
      "language_model.model.layers.10.mlp.act_fn\n",
      "language_model.model.layers.10.input_layernorm\n",
      "language_model.model.layers.10.post_attention_layernorm\n",
      "language_model.model.layers.11\n",
      "language_model.model.layers.11.self_attn\n",
      "language_model.model.layers.11.self_attn.q_proj\n",
      "language_model.model.layers.11.self_attn.k_proj\n",
      "language_model.model.layers.11.self_attn.v_proj\n",
      "language_model.model.layers.11.self_attn.o_proj\n",
      "language_model.model.layers.11.self_attn.rotary_emb\n",
      "language_model.model.layers.11.mlp\n",
      "language_model.model.layers.11.mlp.gate_proj\n",
      "language_model.model.layers.11.mlp.up_proj\n",
      "language_model.model.layers.11.mlp.down_proj\n",
      "language_model.model.layers.11.mlp.act_fn\n",
      "language_model.model.layers.11.input_layernorm\n",
      "language_model.model.layers.11.post_attention_layernorm\n",
      "language_model.model.layers.12\n",
      "language_model.model.layers.12.self_attn\n",
      "language_model.model.layers.12.self_attn.q_proj\n",
      "language_model.model.layers.12.self_attn.k_proj\n",
      "language_model.model.layers.12.self_attn.v_proj\n",
      "language_model.model.layers.12.self_attn.o_proj\n",
      "language_model.model.layers.12.self_attn.rotary_emb\n",
      "language_model.model.layers.12.mlp\n",
      "language_model.model.layers.12.mlp.gate_proj\n",
      "language_model.model.layers.12.mlp.up_proj\n",
      "language_model.model.layers.12.mlp.down_proj\n",
      "language_model.model.layers.12.mlp.act_fn\n",
      "language_model.model.layers.12.input_layernorm\n",
      "language_model.model.layers.12.post_attention_layernorm\n",
      "language_model.model.layers.13\n",
      "language_model.model.layers.13.self_attn\n",
      "language_model.model.layers.13.self_attn.q_proj\n",
      "language_model.model.layers.13.self_attn.k_proj\n",
      "language_model.model.layers.13.self_attn.v_proj\n",
      "language_model.model.layers.13.self_attn.o_proj\n",
      "language_model.model.layers.13.self_attn.rotary_emb\n",
      "language_model.model.layers.13.mlp\n",
      "language_model.model.layers.13.mlp.gate_proj\n",
      "language_model.model.layers.13.mlp.up_proj\n",
      "language_model.model.layers.13.mlp.down_proj\n",
      "language_model.model.layers.13.mlp.act_fn\n",
      "language_model.model.layers.13.input_layernorm\n",
      "language_model.model.layers.13.post_attention_layernorm\n",
      "language_model.model.layers.14\n",
      "language_model.model.layers.14.self_attn\n",
      "language_model.model.layers.14.self_attn.q_proj\n",
      "language_model.model.layers.14.self_attn.k_proj\n",
      "language_model.model.layers.14.self_attn.v_proj\n",
      "language_model.model.layers.14.self_attn.o_proj\n",
      "language_model.model.layers.14.self_attn.rotary_emb\n",
      "language_model.model.layers.14.mlp\n",
      "language_model.model.layers.14.mlp.gate_proj\n",
      "language_model.model.layers.14.mlp.up_proj\n",
      "language_model.model.layers.14.mlp.down_proj\n",
      "language_model.model.layers.14.mlp.act_fn\n",
      "language_model.model.layers.14.input_layernorm\n",
      "language_model.model.layers.14.post_attention_layernorm\n",
      "language_model.model.layers.15\n",
      "language_model.model.layers.15.self_attn\n",
      "language_model.model.layers.15.self_attn.q_proj\n",
      "language_model.model.layers.15.self_attn.k_proj\n",
      "language_model.model.layers.15.self_attn.v_proj\n",
      "language_model.model.layers.15.self_attn.o_proj\n",
      "language_model.model.layers.15.self_attn.rotary_emb\n",
      "language_model.model.layers.15.mlp\n",
      "language_model.model.layers.15.mlp.gate_proj\n",
      "language_model.model.layers.15.mlp.up_proj\n",
      "language_model.model.layers.15.mlp.down_proj\n",
      "language_model.model.layers.15.mlp.act_fn\n",
      "language_model.model.layers.15.input_layernorm\n",
      "language_model.model.layers.15.post_attention_layernorm\n",
      "language_model.model.layers.16\n",
      "language_model.model.layers.16.self_attn\n",
      "language_model.model.layers.16.self_attn.q_proj\n",
      "language_model.model.layers.16.self_attn.k_proj\n",
      "language_model.model.layers.16.self_attn.v_proj\n",
      "language_model.model.layers.16.self_attn.o_proj\n",
      "language_model.model.layers.16.self_attn.rotary_emb\n",
      "language_model.model.layers.16.mlp\n",
      "language_model.model.layers.16.mlp.gate_proj\n",
      "language_model.model.layers.16.mlp.up_proj\n",
      "language_model.model.layers.16.mlp.down_proj\n",
      "language_model.model.layers.16.mlp.act_fn\n",
      "language_model.model.layers.16.input_layernorm\n",
      "language_model.model.layers.16.post_attention_layernorm\n",
      "language_model.model.layers.17\n",
      "language_model.model.layers.17.self_attn\n",
      "language_model.model.layers.17.self_attn.q_proj\n",
      "language_model.model.layers.17.self_attn.k_proj\n",
      "language_model.model.layers.17.self_attn.v_proj\n",
      "language_model.model.layers.17.self_attn.o_proj\n",
      "language_model.model.layers.17.self_attn.rotary_emb\n",
      "language_model.model.layers.17.mlp\n",
      "language_model.model.layers.17.mlp.gate_proj\n",
      "language_model.model.layers.17.mlp.up_proj\n",
      "language_model.model.layers.17.mlp.down_proj\n",
      "language_model.model.layers.17.mlp.act_fn\n",
      "language_model.model.layers.17.input_layernorm\n",
      "language_model.model.layers.17.post_attention_layernorm\n",
      "language_model.model.layers.18\n",
      "language_model.model.layers.18.self_attn\n",
      "language_model.model.layers.18.self_attn.q_proj\n",
      "language_model.model.layers.18.self_attn.k_proj\n",
      "language_model.model.layers.18.self_attn.v_proj\n",
      "language_model.model.layers.18.self_attn.o_proj\n",
      "language_model.model.layers.18.self_attn.rotary_emb\n",
      "language_model.model.layers.18.mlp\n",
      "language_model.model.layers.18.mlp.gate_proj\n",
      "language_model.model.layers.18.mlp.up_proj\n",
      "language_model.model.layers.18.mlp.down_proj\n",
      "language_model.model.layers.18.mlp.act_fn\n",
      "language_model.model.layers.18.input_layernorm\n",
      "language_model.model.layers.18.post_attention_layernorm\n",
      "language_model.model.layers.19\n",
      "language_model.model.layers.19.self_attn\n",
      "language_model.model.layers.19.self_attn.q_proj\n",
      "language_model.model.layers.19.self_attn.k_proj\n",
      "language_model.model.layers.19.self_attn.v_proj\n",
      "language_model.model.layers.19.self_attn.o_proj\n",
      "language_model.model.layers.19.self_attn.rotary_emb\n",
      "language_model.model.layers.19.mlp\n",
      "language_model.model.layers.19.mlp.gate_proj\n",
      "language_model.model.layers.19.mlp.up_proj\n",
      "language_model.model.layers.19.mlp.down_proj\n",
      "language_model.model.layers.19.mlp.act_fn\n",
      "language_model.model.layers.19.input_layernorm\n",
      "language_model.model.layers.19.post_attention_layernorm\n",
      "language_model.model.layers.20\n",
      "language_model.model.layers.20.self_attn\n",
      "language_model.model.layers.20.self_attn.q_proj\n",
      "language_model.model.layers.20.self_attn.k_proj\n",
      "language_model.model.layers.20.self_attn.v_proj\n",
      "language_model.model.layers.20.self_attn.o_proj\n",
      "language_model.model.layers.20.self_attn.rotary_emb\n",
      "language_model.model.layers.20.mlp\n",
      "language_model.model.layers.20.mlp.gate_proj\n",
      "language_model.model.layers.20.mlp.up_proj\n",
      "language_model.model.layers.20.mlp.down_proj\n",
      "language_model.model.layers.20.mlp.act_fn\n",
      "language_model.model.layers.20.input_layernorm\n",
      "language_model.model.layers.20.post_attention_layernorm\n",
      "language_model.model.layers.21\n",
      "language_model.model.layers.21.self_attn\n",
      "language_model.model.layers.21.self_attn.q_proj\n",
      "language_model.model.layers.21.self_attn.k_proj\n",
      "language_model.model.layers.21.self_attn.v_proj\n",
      "language_model.model.layers.21.self_attn.o_proj\n",
      "language_model.model.layers.21.self_attn.rotary_emb\n",
      "language_model.model.layers.21.mlp\n",
      "language_model.model.layers.21.mlp.gate_proj\n",
      "language_model.model.layers.21.mlp.up_proj\n",
      "language_model.model.layers.21.mlp.down_proj\n",
      "language_model.model.layers.21.mlp.act_fn\n",
      "language_model.model.layers.21.input_layernorm\n",
      "language_model.model.layers.21.post_attention_layernorm\n",
      "language_model.model.layers.22\n",
      "language_model.model.layers.22.self_attn\n",
      "language_model.model.layers.22.self_attn.q_proj\n",
      "language_model.model.layers.22.self_attn.k_proj\n",
      "language_model.model.layers.22.self_attn.v_proj\n",
      "language_model.model.layers.22.self_attn.o_proj\n",
      "language_model.model.layers.22.self_attn.rotary_emb\n",
      "language_model.model.layers.22.mlp\n",
      "language_model.model.layers.22.mlp.gate_proj\n",
      "language_model.model.layers.22.mlp.up_proj\n",
      "language_model.model.layers.22.mlp.down_proj\n",
      "language_model.model.layers.22.mlp.act_fn\n",
      "language_model.model.layers.22.input_layernorm\n",
      "language_model.model.layers.22.post_attention_layernorm\n",
      "language_model.model.layers.23\n",
      "language_model.model.layers.23.self_attn\n",
      "language_model.model.layers.23.self_attn.q_proj\n",
      "language_model.model.layers.23.self_attn.k_proj\n",
      "language_model.model.layers.23.self_attn.v_proj\n",
      "language_model.model.layers.23.self_attn.o_proj\n",
      "language_model.model.layers.23.self_attn.rotary_emb\n",
      "language_model.model.layers.23.mlp\n",
      "language_model.model.layers.23.mlp.gate_proj\n",
      "language_model.model.layers.23.mlp.up_proj\n",
      "language_model.model.layers.23.mlp.down_proj\n",
      "language_model.model.layers.23.mlp.act_fn\n",
      "language_model.model.layers.23.input_layernorm\n",
      "language_model.model.layers.23.post_attention_layernorm\n",
      "language_model.model.layers.24\n",
      "language_model.model.layers.24.self_attn\n",
      "language_model.model.layers.24.self_attn.q_proj\n",
      "language_model.model.layers.24.self_attn.k_proj\n",
      "language_model.model.layers.24.self_attn.v_proj\n",
      "language_model.model.layers.24.self_attn.o_proj\n",
      "language_model.model.layers.24.self_attn.rotary_emb\n",
      "language_model.model.layers.24.mlp\n",
      "language_model.model.layers.24.mlp.gate_proj\n",
      "language_model.model.layers.24.mlp.up_proj\n",
      "language_model.model.layers.24.mlp.down_proj\n",
      "language_model.model.layers.24.mlp.act_fn\n",
      "language_model.model.layers.24.input_layernorm\n",
      "language_model.model.layers.24.post_attention_layernorm\n",
      "language_model.model.layers.25\n",
      "language_model.model.layers.25.self_attn\n",
      "language_model.model.layers.25.self_attn.q_proj\n",
      "language_model.model.layers.25.self_attn.k_proj\n",
      "language_model.model.layers.25.self_attn.v_proj\n",
      "language_model.model.layers.25.self_attn.o_proj\n",
      "language_model.model.layers.25.self_attn.rotary_emb\n",
      "language_model.model.layers.25.mlp\n",
      "language_model.model.layers.25.mlp.gate_proj\n",
      "language_model.model.layers.25.mlp.up_proj\n",
      "language_model.model.layers.25.mlp.down_proj\n",
      "language_model.model.layers.25.mlp.act_fn\n",
      "language_model.model.layers.25.input_layernorm\n",
      "language_model.model.layers.25.post_attention_layernorm\n",
      "language_model.model.layers.26\n",
      "language_model.model.layers.26.self_attn\n",
      "language_model.model.layers.26.self_attn.q_proj\n",
      "language_model.model.layers.26.self_attn.k_proj\n",
      "language_model.model.layers.26.self_attn.v_proj\n",
      "language_model.model.layers.26.self_attn.o_proj\n",
      "language_model.model.layers.26.self_attn.rotary_emb\n",
      "language_model.model.layers.26.mlp\n",
      "language_model.model.layers.26.mlp.gate_proj\n",
      "language_model.model.layers.26.mlp.up_proj\n",
      "language_model.model.layers.26.mlp.down_proj\n",
      "language_model.model.layers.26.mlp.act_fn\n",
      "language_model.model.layers.26.input_layernorm\n",
      "language_model.model.layers.26.post_attention_layernorm\n",
      "language_model.model.layers.27\n",
      "language_model.model.layers.27.self_attn\n",
      "language_model.model.layers.27.self_attn.q_proj\n",
      "language_model.model.layers.27.self_attn.k_proj\n",
      "language_model.model.layers.27.self_attn.v_proj\n",
      "language_model.model.layers.27.self_attn.o_proj\n",
      "language_model.model.layers.27.self_attn.rotary_emb\n",
      "language_model.model.layers.27.mlp\n",
      "language_model.model.layers.27.mlp.gate_proj\n",
      "language_model.model.layers.27.mlp.up_proj\n",
      "language_model.model.layers.27.mlp.down_proj\n",
      "language_model.model.layers.27.mlp.act_fn\n",
      "language_model.model.layers.27.input_layernorm\n",
      "language_model.model.layers.27.post_attention_layernorm\n",
      "language_model.model.layers.28\n",
      "language_model.model.layers.28.self_attn\n",
      "language_model.model.layers.28.self_attn.q_proj\n",
      "language_model.model.layers.28.self_attn.k_proj\n",
      "language_model.model.layers.28.self_attn.v_proj\n",
      "language_model.model.layers.28.self_attn.o_proj\n",
      "language_model.model.layers.28.self_attn.rotary_emb\n",
      "language_model.model.layers.28.mlp\n",
      "language_model.model.layers.28.mlp.gate_proj\n",
      "language_model.model.layers.28.mlp.up_proj\n",
      "language_model.model.layers.28.mlp.down_proj\n",
      "language_model.model.layers.28.mlp.act_fn\n",
      "language_model.model.layers.28.input_layernorm\n",
      "language_model.model.layers.28.post_attention_layernorm\n",
      "language_model.model.layers.29\n",
      "language_model.model.layers.29.self_attn\n",
      "language_model.model.layers.29.self_attn.q_proj\n",
      "language_model.model.layers.29.self_attn.k_proj\n",
      "language_model.model.layers.29.self_attn.v_proj\n",
      "language_model.model.layers.29.self_attn.o_proj\n",
      "language_model.model.layers.29.self_attn.rotary_emb\n",
      "language_model.model.layers.29.mlp\n",
      "language_model.model.layers.29.mlp.gate_proj\n",
      "language_model.model.layers.29.mlp.up_proj\n",
      "language_model.model.layers.29.mlp.down_proj\n",
      "language_model.model.layers.29.mlp.act_fn\n",
      "language_model.model.layers.29.input_layernorm\n",
      "language_model.model.layers.29.post_attention_layernorm\n",
      "language_model.model.layers.30\n",
      "language_model.model.layers.30.self_attn\n",
      "language_model.model.layers.30.self_attn.q_proj\n",
      "language_model.model.layers.30.self_attn.k_proj\n",
      "language_model.model.layers.30.self_attn.v_proj\n",
      "language_model.model.layers.30.self_attn.o_proj\n",
      "language_model.model.layers.30.self_attn.rotary_emb\n",
      "language_model.model.layers.30.mlp\n",
      "language_model.model.layers.30.mlp.gate_proj\n",
      "language_model.model.layers.30.mlp.up_proj\n",
      "language_model.model.layers.30.mlp.down_proj\n",
      "language_model.model.layers.30.mlp.act_fn\n",
      "language_model.model.layers.30.input_layernorm\n",
      "language_model.model.layers.30.post_attention_layernorm\n",
      "language_model.model.layers.31\n",
      "language_model.model.layers.31.self_attn\n",
      "language_model.model.layers.31.self_attn.q_proj\n",
      "language_model.model.layers.31.self_attn.k_proj\n",
      "language_model.model.layers.31.self_attn.v_proj\n",
      "language_model.model.layers.31.self_attn.o_proj\n",
      "language_model.model.layers.31.self_attn.rotary_emb\n",
      "language_model.model.layers.31.mlp\n",
      "language_model.model.layers.31.mlp.gate_proj\n",
      "language_model.model.layers.31.mlp.up_proj\n",
      "language_model.model.layers.31.mlp.down_proj\n",
      "language_model.model.layers.31.mlp.act_fn\n",
      "language_model.model.layers.31.input_layernorm\n",
      "language_model.model.layers.31.post_attention_layernorm\n",
      "language_model.model.norm\n",
      "language_model.lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/xl_vlm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:41<00:00, 14.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "# Step 1: Setup constants\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16\n",
    "cache_dir = \"/netscratch/kadir/xl-vlms/cache/hub\"  # Your custom cache directory\n",
    "\n",
    "# Step 2: Load Processor and Model with cache_dir\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"StanfordAIMI/CheXagent-8b\", \n",
    "    cache_dir=cache_dir, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "generation_config = GenerationConfig.from_pretrained(\n",
    "    \"StanfordAIMI/CheXagent-8b\", \n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"StanfordAIMI/CheXagent-8b\", \n",
    "    cache_dir=cache_dir, \n",
    "    torch_dtype=dtype, \n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXagentForConditionalGeneration(\n",
      "  (vision_model): CheXagentVisionModel(\n",
      "    (embeddings): CheXagentVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (encoder): CheXagentEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-39): 40 x CheXagentEncoderLayer(\n",
      "          (self_attn): CheXagentAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
      "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): CheXagentMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
      "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (qformer): CheXagentQFormerModel(\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (encoder): CheXagentQFormerEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (language_projection): Linear(in_features=768, out_features=4096, bias=True)\n",
      "  (language_model): MistralForCausalLM(\n",
      "    (model): MistralModel(\n",
      "      (embed_tokens): Embedding(32000, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x MistralDecoderLayer(\n",
      "          (self_attn): MistralSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): MistralRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): MistralMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shape is: torch.Size([1, 1, 3, 448, 448])\n",
      "The language shape is: torch.Size([1, 25])\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 5\n",
      "}\n",
      "\n",
      "Generated Response:\n",
      "The hilar contours are normal.\n",
      "\n",
      "Layer Information (Name, Weight Shapes, Output Shape):\n",
      "\n",
      "Layer Name                                                   Weight Shapes                            Output Shape\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "vision_model.embeddings.patch_embedding                      [[1408, 3, 14, 14], [1408]]              torch.Size([1, 1408, 32, 32])\n",
      "vision_model.encoder.layers.39.mlp.fc2                       [[1408, 6144], [1408]]                   torch.Size([1, 1025, 1408])\n",
      "vision_model.post_layernorm                                  [[1408], [1408]]                         torch.Size([1, 1408])\n",
      "qformer.encoder.layer.11.output_query.dense                  [[768, 3072], [768]]                     torch.Size([1, 128, 768])\n",
      "language_projection                                          [[4096, 768], [4096]]                    torch.Size([1, 128, 4096])\n",
      "language_model.model.embed_tokens                            [[32000, 4096]]                          torch.Size([5, 1, 4096])\n",
      "language_model.model.layers.31                               [[4096, 4096], [1024, 4096], [1024, 4096], [4096, 4096], [14336, 4096], [14336, 4096], [4096, 14336], [4096], [4096]] Non-Tensor Output\n",
      "language_model.lm_head                                       [[32000, 4096]]                          torch.Size([5, 1, 32000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Load and prepare the image\n",
    "image_path = \"/home/kadir/xl-vlms/playground/images/synpic32933.jpg\"\n",
    "images = [Image.open(image_path).convert(\"RGB\")]\n",
    "\n",
    "# Step 2: Define the prompt and prepare inputs\n",
    "prompt = f'Describe \"Airway\" in 10 sentaance'\n",
    "inputs = processor(\n",
    "    images=images, \n",
    "    text=f\" USER: <s>{prompt} ASSISTANT: <s>\", \n",
    "    return_tensors=\"pt\"\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "# Show the input shapes\n",
    "print(f\"The image shape is: {inputs['pixel_values'].shape}\")\n",
    "print(f\"The language shape is: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Step 3: Initialize hooks and storage for layer output shapes and weights\n",
    "output_shapes = {}  # Dictionary to store layer output shapes\n",
    "weights = {}        # Dictionary to store weight shapes and sizes\n",
    "\n",
    "# Hook function to capture output shapes during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    # Record the output shape of the module\n",
    "    output_shapes[module] = output.shape if isinstance(output, torch.Tensor) else \"Non-Tensor Output\"\n",
    "\n",
    "# Step 4: Register hooks and collect layer weight information\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    # Register hook for each layer to capture the output shape\n",
    "    hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Collect the weight shapes for each layer\n",
    "    layer_weights = []\n",
    "    for param in module.parameters():\n",
    "        layer_weights.append(list(param.shape))  # Record parameter shapes\n",
    "    weights[name] = layer_weights if layer_weights else \"No Weights\"\n",
    "\n",
    "# Step 5: Perform a forward pass with the model and generate the response\n",
    "print(generation_config)\n",
    "#generation_config.num_beams =1\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# Decode and print the generated response\n",
    "response = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Generated Response:\\n{response}\")\n",
    "\n",
    "# Step 6: Print the layer information (name, weight shapes, and output shape)\n",
    "print(\"\\nLayer Information (Name, Weight Shapes, Output Shape):\\n\")\n",
    "print(f\"{'Layer Name':<60} {'Weight Shapes':<40} {'Output Shape'}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "# Filter and print only the relevant layers for analysis\n",
    "layers_of_interest = [\"vision_model.embeddings.patch_embedding\",\"vision_model.encoder.layers.39.mlp.fc2\", \"vision_model.post_layernorm\", \"language_model.model.embed_tokens\", \"qformer.encoder.layer.11.output_query.dense\", \"language_projection\",  \"language_model.model.embed_tokens\", \"language_model.model.layers.31\",\"language_model.lm_head\"]\n",
    "for module, shape in output_shapes.items():\n",
    "    # Get the layer name by searching in the model's named modules\n",
    "    layer_name = next((name for name, m in model.named_modules() if m == module), \"Unknown\")\n",
    "    \n",
    "    # If the layer is one of the layers of interest, print its info\n",
    "    if layer_name in layers_of_interest:\n",
    "        weight_shapes = str(weights[layer_name]) if weights[layer_name] != \"No Weights\" else \"No Weights\"\n",
    "        print(f\"{layer_name:<60} {weight_shapes:<40} {shape}\")\n",
    "\n",
    "# Clean up hooks to avoid memory leaks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qformer outptut = (128, 768)\n",
    "language_projecton = (128, 4096)\n",
    "text_embedding = (numbe_of_tokens, 4096)\n",
    "inputs_embeds = torch.cat([input_vis, inputs_lang], dim=1) shape (batch,numbe_of_tokens+128, 4096 )\n",
    "language_model.model.layers.31/language_model.model.norm = [1, 128+num_token, 4096]\n",
    "\n",
    "output = [number of ouput token]\n",
    "# step 5: conditioned on the images and/or prompts\n",
    "        if pixel_values is not None:\n",
    "            inputs_embeds = torch.cat([input_vis, inputs_lang], dim=1)\n",
    "            attention_mask = torch.cat([vis_atts, lang_atts], dim=1)\n",
    "        else:\n",
    "            inputs_embeds = inputs_lang\n",
    "            attention_mask = lang_atts\n",
    "\n",
    "        outputs = self.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            **generate_kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXagentForConditionalGeneration(\n",
      "  (vision_model): CheXagentVisionModel(\n",
      "    (embeddings): CheXagentVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (encoder): CheXagentEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-39): 40 x CheXagentEncoderLayer(\n",
      "          (self_attn): CheXagentAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
      "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): CheXagentMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
      "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (qformer): CheXagentQFormerModel(\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (encoder): CheXagentQFormerEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (language_projection): Linear(in_features=768, out_features=4096, bias=True)\n",
      "  (language_model): MistralForCausalLM(\n",
      "    (model): MistralModel(\n",
      "      (embed_tokens): Embedding(32000, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x MistralDecoderLayer(\n",
      "          (self_attn): MistralSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): MistralRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): MistralMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/kadir/xl-vlms/playground/images/synpic32933.jpg\"\n",
    "images = [Image.open(image_path).convert(\"RGB\")]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/StanfordAIMI/CheXagent-8b/4934e91451945c8218c267aae9c34929a7677829/processing_chexagent.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(pixel_values) for pixel_values in encoding_image_processor[\"pixel_values\"]]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = f'Describe \"Airway\", '\n",
    "inputs = processor(images=images, text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\").to(device=device, dtype=dtype)\n",
    "model = model.to(device)\n",
    "output = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "response = processor.tokenizer.decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hilar contours are normal.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "from models.image_text_model import ImageTextModel\n",
    "import logging\n",
    "\n",
    "__all__ = [\"Molmo\"]\n",
    "\n",
    "\n",
    "class Molmo(ImageTextModel):\n",
    "\n",
    "    def set_model(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        logging.debug(\"This is a debug message (useful for detailed troubleshooting).\")\n",
    "        self.model_ = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            local_files_only=self.local_files_only,\n",
    "        )\n",
    "\n",
    "    def get_model(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_language_model(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self.model_.model.transformer\n",
    "\n",
    "    def get_lm_head(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self.model_.model.transformer.ff_out\n",
    "\n",
    "    def set_processor(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.processor_ = AutoProcessor.from_pretrained(\n",
    "            self.processor_name,\n",
    "            local_files_only=self.local_files_only,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        self.tokenizer_ = self.processor_.tokenizer\n",
    "\n",
    "    def set_preprocessor(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.preprocessor_ = self.preprocess_input\n",
    "\n",
    "    def get_conversation_template(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        response: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        conversation = instruction\n",
    "        if response:\n",
    "            conversation += f\" Answer: {response}\"\n",
    "        return conversation\n",
    "\n",
    "    def preprocess_input(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        image_file: str = None,\n",
    "        response: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        text = self.get_conversation_template(\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "            image_file=image_file,\n",
    "        )\n",
    "\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        inputs = self.processor_.process(\n",
    "            text=text,\n",
    "            images=[image],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def preprocessor(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        image_file: str = \"\",\n",
    "        response: str = \"\",\n",
    "        generation_mode: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        preprocessor = self.get_preprocessor()\n",
    "        inputs = preprocessor(\n",
    "            instruction=instruction,\n",
    "            image_file=image_file,\n",
    "            response=response,\n",
    "            generation_mode=generation_mode,\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        max_new_tokens: int = 200,\n",
    "        do_sample: bool = False,\n",
    "        **inputs: Dict[str, Any],\n",
    "    ):\n",
    "        inputs = {k: v.unsqueeze(0).to(self.model_.device) for k, v in inputs.items()}\n",
    "        device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with torch.autocast(\n",
    "            device_type=device_type, enabled=True, dtype=self.model_.dtype\n",
    "        ):\n",
    "            output = self.model_.generate_from_batch(\n",
    "                inputs,\n",
    "                GenerationConfig(\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    stop_strings=\"<|endoftext|>\",\n",
    "                    do_sample=do_sample,\n",
    "                ),\n",
    "                tokenizer=self.tokenizer_,\n",
    "            )\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "from models.image_text_model import ImageTextModel\n",
    "\n",
    "__all__ = [\"ChexAgent\"]\n",
    "\n",
    "\n",
    "class ChexAgent(ImageTextModel):\n",
    "\n",
    "    def set_model(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.model_ = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            cache_dir ='/netscratch/kadir/xl-vlms/cache/hub',\n",
    "            local_files_only=self.local_files_only,\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_language_model(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self.model_.model\n",
    "\n",
    "    def get_lm_head(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self.model_.lm_head\n",
    "\n",
    "    def set_processor(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.processor_ = AutoProcessor.from_pretrained(\n",
    "            self.processor_name,\n",
    "            local_files_only=self.local_files_only,\n",
    "        )\n",
    "        self.tokenizer_ = self.processor_.tokenizer\n",
    "\n",
    "    def set_preprocessor(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.preprocessor_ = self.preprocess_input\n",
    "\n",
    "    def get_conversation_round(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        response: str = \"\",\n",
    "        image_file: str = \"\",\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": image_file,\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": instruction},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        if response:\n",
    "            conversation.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": response},\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "\n",
    "        return conversation\n",
    "\n",
    "    def get_conversation_template(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        response: str = \"\",\n",
    "        image_file: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        conversation = self.get_conversation_round(\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "            image_file=image_file,\n",
    "        )\n",
    "        return conversation\n",
    "\n",
    "    def preprocess_text(\n",
    "        self,\n",
    "        conversation,\n",
    "        generation_mode: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "\n",
    "        prompt = self.processor_.apply_chat_template(\n",
    "            conversation,\n",
    "            add_generation_prompt=generation_mode,\n",
    "            tokenize=False,\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def preprocess_images(\n",
    "        self,\n",
    "        conversation,\n",
    "        **kwargs: Any,\n",
    "    ) -> List:\n",
    "\n",
    "        image_inputs, video_inputs = process_vision_info(conversation)\n",
    "\n",
    "        return image_inputs, video_inputs\n",
    "\n",
    "    def preprocess_input(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        image_file: str = None,\n",
    "        response: str = \"\",\n",
    "        generation_mode: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        conversation = self.get_conversation_template(\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "            image_file=image_file,\n",
    "        )\n",
    "\n",
    "        image, video = self.preprocess_images(conversation)\n",
    "        text = self.preprocess_text(\n",
    "            conversation,\n",
    "            generation_mode=generation_mode,\n",
    "        )\n",
    "\n",
    "        inputs = self.processor_(\n",
    "            text=[text],\n",
    "            images=image,\n",
    "            videos=video,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:12<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name= \"StanfordAIMI/CheXagent-8b\"\n",
    "\n",
    "model_class = ChexAgent(\n",
    "            model_name_or_path=model_name,\n",
    "            cache_dir = '/netscratch/kadir/xl-vlms/cache/hub',\n",
    "            processor_name=model_name,\n",
    "            local_files_only=False,\n",
    "            trust_remote_code=True,\n",
    "        \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version for info: 3.9.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "print(\"Matplotlib version for info:\", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10368)\n"
     ]
    }
   ],
   "source": [
    "results_dict = torch.load('/netscratch/kadir/xl-vlms/features/save_hidden_states_for_token_of_interest_chexagent_A_generation_split_train.pth')\n",
    "#print(results_dict['model_predictions'])\n",
    "\n",
    "print(torch.tensor(results_dict['token_of_interest_mask']).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xl_vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
