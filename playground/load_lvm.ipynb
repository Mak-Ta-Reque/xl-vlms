{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/xl_vlm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/netscratch/kadir/xl-vlms/cache'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Setup constant\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [02:34<00:00, 22.12s/it]\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_HOME'] = '/netscratch/kadir/xl-vlms/cache/hub'\n",
    "processor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", cache_dir = '/netscratch/kadir/xl-vlms/cache/hub',  trust_remote_code=True)\n",
    "generation_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"StanfordAIMI/CheXagent-8b\", cache_dir = '/netscratch/kadir/xl-vlms/cache/hub', torch_dtype=dtype, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchinfo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchinfo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary(model, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(model, depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vision_model\n",
      "vision_model.embeddings\n",
      "vision_model.embeddings.patch_embedding\n",
      "vision_model.encoder\n",
      "vision_model.encoder.layers\n",
      "vision_model.encoder.layers.0\n",
      "vision_model.encoder.layers.0.self_attn\n",
      "vision_model.encoder.layers.0.self_attn.dropout\n",
      "vision_model.encoder.layers.0.self_attn.qkv\n",
      "vision_model.encoder.layers.0.self_attn.projection\n",
      "vision_model.encoder.layers.0.layer_norm1\n",
      "vision_model.encoder.layers.0.mlp\n",
      "vision_model.encoder.layers.0.mlp.activation_fn\n",
      "vision_model.encoder.layers.0.mlp.fc1\n",
      "vision_model.encoder.layers.0.mlp.fc2\n",
      "vision_model.encoder.layers.0.layer_norm2\n",
      "vision_model.encoder.layers.1\n",
      "vision_model.encoder.layers.1.self_attn\n",
      "vision_model.encoder.layers.1.self_attn.dropout\n",
      "vision_model.encoder.layers.1.self_attn.qkv\n",
      "vision_model.encoder.layers.1.self_attn.projection\n",
      "vision_model.encoder.layers.1.layer_norm1\n",
      "vision_model.encoder.layers.1.mlp\n",
      "vision_model.encoder.layers.1.mlp.activation_fn\n",
      "vision_model.encoder.layers.1.mlp.fc1\n",
      "vision_model.encoder.layers.1.mlp.fc2\n",
      "vision_model.encoder.layers.1.layer_norm2\n",
      "vision_model.encoder.layers.2\n",
      "vision_model.encoder.layers.2.self_attn\n",
      "vision_model.encoder.layers.2.self_attn.dropout\n",
      "vision_model.encoder.layers.2.self_attn.qkv\n",
      "vision_model.encoder.layers.2.self_attn.projection\n",
      "vision_model.encoder.layers.2.layer_norm1\n",
      "vision_model.encoder.layers.2.mlp\n",
      "vision_model.encoder.layers.2.mlp.activation_fn\n",
      "vision_model.encoder.layers.2.mlp.fc1\n",
      "vision_model.encoder.layers.2.mlp.fc2\n",
      "vision_model.encoder.layers.2.layer_norm2\n",
      "vision_model.encoder.layers.3\n",
      "vision_model.encoder.layers.3.self_attn\n",
      "vision_model.encoder.layers.3.self_attn.dropout\n",
      "vision_model.encoder.layers.3.self_attn.qkv\n",
      "vision_model.encoder.layers.3.self_attn.projection\n",
      "vision_model.encoder.layers.3.layer_norm1\n",
      "vision_model.encoder.layers.3.mlp\n",
      "vision_model.encoder.layers.3.mlp.activation_fn\n",
      "vision_model.encoder.layers.3.mlp.fc1\n",
      "vision_model.encoder.layers.3.mlp.fc2\n",
      "vision_model.encoder.layers.3.layer_norm2\n",
      "vision_model.encoder.layers.4\n",
      "vision_model.encoder.layers.4.self_attn\n",
      "vision_model.encoder.layers.4.self_attn.dropout\n",
      "vision_model.encoder.layers.4.self_attn.qkv\n",
      "vision_model.encoder.layers.4.self_attn.projection\n",
      "vision_model.encoder.layers.4.layer_norm1\n",
      "vision_model.encoder.layers.4.mlp\n",
      "vision_model.encoder.layers.4.mlp.activation_fn\n",
      "vision_model.encoder.layers.4.mlp.fc1\n",
      "vision_model.encoder.layers.4.mlp.fc2\n",
      "vision_model.encoder.layers.4.layer_norm2\n",
      "vision_model.encoder.layers.5\n",
      "vision_model.encoder.layers.5.self_attn\n",
      "vision_model.encoder.layers.5.self_attn.dropout\n",
      "vision_model.encoder.layers.5.self_attn.qkv\n",
      "vision_model.encoder.layers.5.self_attn.projection\n",
      "vision_model.encoder.layers.5.layer_norm1\n",
      "vision_model.encoder.layers.5.mlp\n",
      "vision_model.encoder.layers.5.mlp.activation_fn\n",
      "vision_model.encoder.layers.5.mlp.fc1\n",
      "vision_model.encoder.layers.5.mlp.fc2\n",
      "vision_model.encoder.layers.5.layer_norm2\n",
      "vision_model.encoder.layers.6\n",
      "vision_model.encoder.layers.6.self_attn\n",
      "vision_model.encoder.layers.6.self_attn.dropout\n",
      "vision_model.encoder.layers.6.self_attn.qkv\n",
      "vision_model.encoder.layers.6.self_attn.projection\n",
      "vision_model.encoder.layers.6.layer_norm1\n",
      "vision_model.encoder.layers.6.mlp\n",
      "vision_model.encoder.layers.6.mlp.activation_fn\n",
      "vision_model.encoder.layers.6.mlp.fc1\n",
      "vision_model.encoder.layers.6.mlp.fc2\n",
      "vision_model.encoder.layers.6.layer_norm2\n",
      "vision_model.encoder.layers.7\n",
      "vision_model.encoder.layers.7.self_attn\n",
      "vision_model.encoder.layers.7.self_attn.dropout\n",
      "vision_model.encoder.layers.7.self_attn.qkv\n",
      "vision_model.encoder.layers.7.self_attn.projection\n",
      "vision_model.encoder.layers.7.layer_norm1\n",
      "vision_model.encoder.layers.7.mlp\n",
      "vision_model.encoder.layers.7.mlp.activation_fn\n",
      "vision_model.encoder.layers.7.mlp.fc1\n",
      "vision_model.encoder.layers.7.mlp.fc2\n",
      "vision_model.encoder.layers.7.layer_norm2\n",
      "vision_model.encoder.layers.8\n",
      "vision_model.encoder.layers.8.self_attn\n",
      "vision_model.encoder.layers.8.self_attn.dropout\n",
      "vision_model.encoder.layers.8.self_attn.qkv\n",
      "vision_model.encoder.layers.8.self_attn.projection\n",
      "vision_model.encoder.layers.8.layer_norm1\n",
      "vision_model.encoder.layers.8.mlp\n",
      "vision_model.encoder.layers.8.mlp.activation_fn\n",
      "vision_model.encoder.layers.8.mlp.fc1\n",
      "vision_model.encoder.layers.8.mlp.fc2\n",
      "vision_model.encoder.layers.8.layer_norm2\n",
      "vision_model.encoder.layers.9\n",
      "vision_model.encoder.layers.9.self_attn\n",
      "vision_model.encoder.layers.9.self_attn.dropout\n",
      "vision_model.encoder.layers.9.self_attn.qkv\n",
      "vision_model.encoder.layers.9.self_attn.projection\n",
      "vision_model.encoder.layers.9.layer_norm1\n",
      "vision_model.encoder.layers.9.mlp\n",
      "vision_model.encoder.layers.9.mlp.activation_fn\n",
      "vision_model.encoder.layers.9.mlp.fc1\n",
      "vision_model.encoder.layers.9.mlp.fc2\n",
      "vision_model.encoder.layers.9.layer_norm2\n",
      "vision_model.encoder.layers.10\n",
      "vision_model.encoder.layers.10.self_attn\n",
      "vision_model.encoder.layers.10.self_attn.dropout\n",
      "vision_model.encoder.layers.10.self_attn.qkv\n",
      "vision_model.encoder.layers.10.self_attn.projection\n",
      "vision_model.encoder.layers.10.layer_norm1\n",
      "vision_model.encoder.layers.10.mlp\n",
      "vision_model.encoder.layers.10.mlp.activation_fn\n",
      "vision_model.encoder.layers.10.mlp.fc1\n",
      "vision_model.encoder.layers.10.mlp.fc2\n",
      "vision_model.encoder.layers.10.layer_norm2\n",
      "vision_model.encoder.layers.11\n",
      "vision_model.encoder.layers.11.self_attn\n",
      "vision_model.encoder.layers.11.self_attn.dropout\n",
      "vision_model.encoder.layers.11.self_attn.qkv\n",
      "vision_model.encoder.layers.11.self_attn.projection\n",
      "vision_model.encoder.layers.11.layer_norm1\n",
      "vision_model.encoder.layers.11.mlp\n",
      "vision_model.encoder.layers.11.mlp.activation_fn\n",
      "vision_model.encoder.layers.11.mlp.fc1\n",
      "vision_model.encoder.layers.11.mlp.fc2\n",
      "vision_model.encoder.layers.11.layer_norm2\n",
      "vision_model.encoder.layers.12\n",
      "vision_model.encoder.layers.12.self_attn\n",
      "vision_model.encoder.layers.12.self_attn.dropout\n",
      "vision_model.encoder.layers.12.self_attn.qkv\n",
      "vision_model.encoder.layers.12.self_attn.projection\n",
      "vision_model.encoder.layers.12.layer_norm1\n",
      "vision_model.encoder.layers.12.mlp\n",
      "vision_model.encoder.layers.12.mlp.activation_fn\n",
      "vision_model.encoder.layers.12.mlp.fc1\n",
      "vision_model.encoder.layers.12.mlp.fc2\n",
      "vision_model.encoder.layers.12.layer_norm2\n",
      "vision_model.encoder.layers.13\n",
      "vision_model.encoder.layers.13.self_attn\n",
      "vision_model.encoder.layers.13.self_attn.dropout\n",
      "vision_model.encoder.layers.13.self_attn.qkv\n",
      "vision_model.encoder.layers.13.self_attn.projection\n",
      "vision_model.encoder.layers.13.layer_norm1\n",
      "vision_model.encoder.layers.13.mlp\n",
      "vision_model.encoder.layers.13.mlp.activation_fn\n",
      "vision_model.encoder.layers.13.mlp.fc1\n",
      "vision_model.encoder.layers.13.mlp.fc2\n",
      "vision_model.encoder.layers.13.layer_norm2\n",
      "vision_model.encoder.layers.14\n",
      "vision_model.encoder.layers.14.self_attn\n",
      "vision_model.encoder.layers.14.self_attn.dropout\n",
      "vision_model.encoder.layers.14.self_attn.qkv\n",
      "vision_model.encoder.layers.14.self_attn.projection\n",
      "vision_model.encoder.layers.14.layer_norm1\n",
      "vision_model.encoder.layers.14.mlp\n",
      "vision_model.encoder.layers.14.mlp.activation_fn\n",
      "vision_model.encoder.layers.14.mlp.fc1\n",
      "vision_model.encoder.layers.14.mlp.fc2\n",
      "vision_model.encoder.layers.14.layer_norm2\n",
      "vision_model.encoder.layers.15\n",
      "vision_model.encoder.layers.15.self_attn\n",
      "vision_model.encoder.layers.15.self_attn.dropout\n",
      "vision_model.encoder.layers.15.self_attn.qkv\n",
      "vision_model.encoder.layers.15.self_attn.projection\n",
      "vision_model.encoder.layers.15.layer_norm1\n",
      "vision_model.encoder.layers.15.mlp\n",
      "vision_model.encoder.layers.15.mlp.activation_fn\n",
      "vision_model.encoder.layers.15.mlp.fc1\n",
      "vision_model.encoder.layers.15.mlp.fc2\n",
      "vision_model.encoder.layers.15.layer_norm2\n",
      "vision_model.encoder.layers.16\n",
      "vision_model.encoder.layers.16.self_attn\n",
      "vision_model.encoder.layers.16.self_attn.dropout\n",
      "vision_model.encoder.layers.16.self_attn.qkv\n",
      "vision_model.encoder.layers.16.self_attn.projection\n",
      "vision_model.encoder.layers.16.layer_norm1\n",
      "vision_model.encoder.layers.16.mlp\n",
      "vision_model.encoder.layers.16.mlp.activation_fn\n",
      "vision_model.encoder.layers.16.mlp.fc1\n",
      "vision_model.encoder.layers.16.mlp.fc2\n",
      "vision_model.encoder.layers.16.layer_norm2\n",
      "vision_model.encoder.layers.17\n",
      "vision_model.encoder.layers.17.self_attn\n",
      "vision_model.encoder.layers.17.self_attn.dropout\n",
      "vision_model.encoder.layers.17.self_attn.qkv\n",
      "vision_model.encoder.layers.17.self_attn.projection\n",
      "vision_model.encoder.layers.17.layer_norm1\n",
      "vision_model.encoder.layers.17.mlp\n",
      "vision_model.encoder.layers.17.mlp.activation_fn\n",
      "vision_model.encoder.layers.17.mlp.fc1\n",
      "vision_model.encoder.layers.17.mlp.fc2\n",
      "vision_model.encoder.layers.17.layer_norm2\n",
      "vision_model.encoder.layers.18\n",
      "vision_model.encoder.layers.18.self_attn\n",
      "vision_model.encoder.layers.18.self_attn.dropout\n",
      "vision_model.encoder.layers.18.self_attn.qkv\n",
      "vision_model.encoder.layers.18.self_attn.projection\n",
      "vision_model.encoder.layers.18.layer_norm1\n",
      "vision_model.encoder.layers.18.mlp\n",
      "vision_model.encoder.layers.18.mlp.activation_fn\n",
      "vision_model.encoder.layers.18.mlp.fc1\n",
      "vision_model.encoder.layers.18.mlp.fc2\n",
      "vision_model.encoder.layers.18.layer_norm2\n",
      "vision_model.encoder.layers.19\n",
      "vision_model.encoder.layers.19.self_attn\n",
      "vision_model.encoder.layers.19.self_attn.dropout\n",
      "vision_model.encoder.layers.19.self_attn.qkv\n",
      "vision_model.encoder.layers.19.self_attn.projection\n",
      "vision_model.encoder.layers.19.layer_norm1\n",
      "vision_model.encoder.layers.19.mlp\n",
      "vision_model.encoder.layers.19.mlp.activation_fn\n",
      "vision_model.encoder.layers.19.mlp.fc1\n",
      "vision_model.encoder.layers.19.mlp.fc2\n",
      "vision_model.encoder.layers.19.layer_norm2\n",
      "vision_model.encoder.layers.20\n",
      "vision_model.encoder.layers.20.self_attn\n",
      "vision_model.encoder.layers.20.self_attn.dropout\n",
      "vision_model.encoder.layers.20.self_attn.qkv\n",
      "vision_model.encoder.layers.20.self_attn.projection\n",
      "vision_model.encoder.layers.20.layer_norm1\n",
      "vision_model.encoder.layers.20.mlp\n",
      "vision_model.encoder.layers.20.mlp.activation_fn\n",
      "vision_model.encoder.layers.20.mlp.fc1\n",
      "vision_model.encoder.layers.20.mlp.fc2\n",
      "vision_model.encoder.layers.20.layer_norm2\n",
      "vision_model.encoder.layers.21\n",
      "vision_model.encoder.layers.21.self_attn\n",
      "vision_model.encoder.layers.21.self_attn.dropout\n",
      "vision_model.encoder.layers.21.self_attn.qkv\n",
      "vision_model.encoder.layers.21.self_attn.projection\n",
      "vision_model.encoder.layers.21.layer_norm1\n",
      "vision_model.encoder.layers.21.mlp\n",
      "vision_model.encoder.layers.21.mlp.activation_fn\n",
      "vision_model.encoder.layers.21.mlp.fc1\n",
      "vision_model.encoder.layers.21.mlp.fc2\n",
      "vision_model.encoder.layers.21.layer_norm2\n",
      "vision_model.encoder.layers.22\n",
      "vision_model.encoder.layers.22.self_attn\n",
      "vision_model.encoder.layers.22.self_attn.dropout\n",
      "vision_model.encoder.layers.22.self_attn.qkv\n",
      "vision_model.encoder.layers.22.self_attn.projection\n",
      "vision_model.encoder.layers.22.layer_norm1\n",
      "vision_model.encoder.layers.22.mlp\n",
      "vision_model.encoder.layers.22.mlp.activation_fn\n",
      "vision_model.encoder.layers.22.mlp.fc1\n",
      "vision_model.encoder.layers.22.mlp.fc2\n",
      "vision_model.encoder.layers.22.layer_norm2\n",
      "vision_model.encoder.layers.23\n",
      "vision_model.encoder.layers.23.self_attn\n",
      "vision_model.encoder.layers.23.self_attn.dropout\n",
      "vision_model.encoder.layers.23.self_attn.qkv\n",
      "vision_model.encoder.layers.23.self_attn.projection\n",
      "vision_model.encoder.layers.23.layer_norm1\n",
      "vision_model.encoder.layers.23.mlp\n",
      "vision_model.encoder.layers.23.mlp.activation_fn\n",
      "vision_model.encoder.layers.23.mlp.fc1\n",
      "vision_model.encoder.layers.23.mlp.fc2\n",
      "vision_model.encoder.layers.23.layer_norm2\n",
      "vision_model.encoder.layers.24\n",
      "vision_model.encoder.layers.24.self_attn\n",
      "vision_model.encoder.layers.24.self_attn.dropout\n",
      "vision_model.encoder.layers.24.self_attn.qkv\n",
      "vision_model.encoder.layers.24.self_attn.projection\n",
      "vision_model.encoder.layers.24.layer_norm1\n",
      "vision_model.encoder.layers.24.mlp\n",
      "vision_model.encoder.layers.24.mlp.activation_fn\n",
      "vision_model.encoder.layers.24.mlp.fc1\n",
      "vision_model.encoder.layers.24.mlp.fc2\n",
      "vision_model.encoder.layers.24.layer_norm2\n",
      "vision_model.encoder.layers.25\n",
      "vision_model.encoder.layers.25.self_attn\n",
      "vision_model.encoder.layers.25.self_attn.dropout\n",
      "vision_model.encoder.layers.25.self_attn.qkv\n",
      "vision_model.encoder.layers.25.self_attn.projection\n",
      "vision_model.encoder.layers.25.layer_norm1\n",
      "vision_model.encoder.layers.25.mlp\n",
      "vision_model.encoder.layers.25.mlp.activation_fn\n",
      "vision_model.encoder.layers.25.mlp.fc1\n",
      "vision_model.encoder.layers.25.mlp.fc2\n",
      "vision_model.encoder.layers.25.layer_norm2\n",
      "vision_model.encoder.layers.26\n",
      "vision_model.encoder.layers.26.self_attn\n",
      "vision_model.encoder.layers.26.self_attn.dropout\n",
      "vision_model.encoder.layers.26.self_attn.qkv\n",
      "vision_model.encoder.layers.26.self_attn.projection\n",
      "vision_model.encoder.layers.26.layer_norm1\n",
      "vision_model.encoder.layers.26.mlp\n",
      "vision_model.encoder.layers.26.mlp.activation_fn\n",
      "vision_model.encoder.layers.26.mlp.fc1\n",
      "vision_model.encoder.layers.26.mlp.fc2\n",
      "vision_model.encoder.layers.26.layer_norm2\n",
      "vision_model.encoder.layers.27\n",
      "vision_model.encoder.layers.27.self_attn\n",
      "vision_model.encoder.layers.27.self_attn.dropout\n",
      "vision_model.encoder.layers.27.self_attn.qkv\n",
      "vision_model.encoder.layers.27.self_attn.projection\n",
      "vision_model.encoder.layers.27.layer_norm1\n",
      "vision_model.encoder.layers.27.mlp\n",
      "vision_model.encoder.layers.27.mlp.activation_fn\n",
      "vision_model.encoder.layers.27.mlp.fc1\n",
      "vision_model.encoder.layers.27.mlp.fc2\n",
      "vision_model.encoder.layers.27.layer_norm2\n",
      "vision_model.encoder.layers.28\n",
      "vision_model.encoder.layers.28.self_attn\n",
      "vision_model.encoder.layers.28.self_attn.dropout\n",
      "vision_model.encoder.layers.28.self_attn.qkv\n",
      "vision_model.encoder.layers.28.self_attn.projection\n",
      "vision_model.encoder.layers.28.layer_norm1\n",
      "vision_model.encoder.layers.28.mlp\n",
      "vision_model.encoder.layers.28.mlp.activation_fn\n",
      "vision_model.encoder.layers.28.mlp.fc1\n",
      "vision_model.encoder.layers.28.mlp.fc2\n",
      "vision_model.encoder.layers.28.layer_norm2\n",
      "vision_model.encoder.layers.29\n",
      "vision_model.encoder.layers.29.self_attn\n",
      "vision_model.encoder.layers.29.self_attn.dropout\n",
      "vision_model.encoder.layers.29.self_attn.qkv\n",
      "vision_model.encoder.layers.29.self_attn.projection\n",
      "vision_model.encoder.layers.29.layer_norm1\n",
      "vision_model.encoder.layers.29.mlp\n",
      "vision_model.encoder.layers.29.mlp.activation_fn\n",
      "vision_model.encoder.layers.29.mlp.fc1\n",
      "vision_model.encoder.layers.29.mlp.fc2\n",
      "vision_model.encoder.layers.29.layer_norm2\n",
      "vision_model.encoder.layers.30\n",
      "vision_model.encoder.layers.30.self_attn\n",
      "vision_model.encoder.layers.30.self_attn.dropout\n",
      "vision_model.encoder.layers.30.self_attn.qkv\n",
      "vision_model.encoder.layers.30.self_attn.projection\n",
      "vision_model.encoder.layers.30.layer_norm1\n",
      "vision_model.encoder.layers.30.mlp\n",
      "vision_model.encoder.layers.30.mlp.activation_fn\n",
      "vision_model.encoder.layers.30.mlp.fc1\n",
      "vision_model.encoder.layers.30.mlp.fc2\n",
      "vision_model.encoder.layers.30.layer_norm2\n",
      "vision_model.encoder.layers.31\n",
      "vision_model.encoder.layers.31.self_attn\n",
      "vision_model.encoder.layers.31.self_attn.dropout\n",
      "vision_model.encoder.layers.31.self_attn.qkv\n",
      "vision_model.encoder.layers.31.self_attn.projection\n",
      "vision_model.encoder.layers.31.layer_norm1\n",
      "vision_model.encoder.layers.31.mlp\n",
      "vision_model.encoder.layers.31.mlp.activation_fn\n",
      "vision_model.encoder.layers.31.mlp.fc1\n",
      "vision_model.encoder.layers.31.mlp.fc2\n",
      "vision_model.encoder.layers.31.layer_norm2\n",
      "vision_model.encoder.layers.32\n",
      "vision_model.encoder.layers.32.self_attn\n",
      "vision_model.encoder.layers.32.self_attn.dropout\n",
      "vision_model.encoder.layers.32.self_attn.qkv\n",
      "vision_model.encoder.layers.32.self_attn.projection\n",
      "vision_model.encoder.layers.32.layer_norm1\n",
      "vision_model.encoder.layers.32.mlp\n",
      "vision_model.encoder.layers.32.mlp.activation_fn\n",
      "vision_model.encoder.layers.32.mlp.fc1\n",
      "vision_model.encoder.layers.32.mlp.fc2\n",
      "vision_model.encoder.layers.32.layer_norm2\n",
      "vision_model.encoder.layers.33\n",
      "vision_model.encoder.layers.33.self_attn\n",
      "vision_model.encoder.layers.33.self_attn.dropout\n",
      "vision_model.encoder.layers.33.self_attn.qkv\n",
      "vision_model.encoder.layers.33.self_attn.projection\n",
      "vision_model.encoder.layers.33.layer_norm1\n",
      "vision_model.encoder.layers.33.mlp\n",
      "vision_model.encoder.layers.33.mlp.activation_fn\n",
      "vision_model.encoder.layers.33.mlp.fc1\n",
      "vision_model.encoder.layers.33.mlp.fc2\n",
      "vision_model.encoder.layers.33.layer_norm2\n",
      "vision_model.encoder.layers.34\n",
      "vision_model.encoder.layers.34.self_attn\n",
      "vision_model.encoder.layers.34.self_attn.dropout\n",
      "vision_model.encoder.layers.34.self_attn.qkv\n",
      "vision_model.encoder.layers.34.self_attn.projection\n",
      "vision_model.encoder.layers.34.layer_norm1\n",
      "vision_model.encoder.layers.34.mlp\n",
      "vision_model.encoder.layers.34.mlp.activation_fn\n",
      "vision_model.encoder.layers.34.mlp.fc1\n",
      "vision_model.encoder.layers.34.mlp.fc2\n",
      "vision_model.encoder.layers.34.layer_norm2\n",
      "vision_model.encoder.layers.35\n",
      "vision_model.encoder.layers.35.self_attn\n",
      "vision_model.encoder.layers.35.self_attn.dropout\n",
      "vision_model.encoder.layers.35.self_attn.qkv\n",
      "vision_model.encoder.layers.35.self_attn.projection\n",
      "vision_model.encoder.layers.35.layer_norm1\n",
      "vision_model.encoder.layers.35.mlp\n",
      "vision_model.encoder.layers.35.mlp.activation_fn\n",
      "vision_model.encoder.layers.35.mlp.fc1\n",
      "vision_model.encoder.layers.35.mlp.fc2\n",
      "vision_model.encoder.layers.35.layer_norm2\n",
      "vision_model.encoder.layers.36\n",
      "vision_model.encoder.layers.36.self_attn\n",
      "vision_model.encoder.layers.36.self_attn.dropout\n",
      "vision_model.encoder.layers.36.self_attn.qkv\n",
      "vision_model.encoder.layers.36.self_attn.projection\n",
      "vision_model.encoder.layers.36.layer_norm1\n",
      "vision_model.encoder.layers.36.mlp\n",
      "vision_model.encoder.layers.36.mlp.activation_fn\n",
      "vision_model.encoder.layers.36.mlp.fc1\n",
      "vision_model.encoder.layers.36.mlp.fc2\n",
      "vision_model.encoder.layers.36.layer_norm2\n",
      "vision_model.encoder.layers.37\n",
      "vision_model.encoder.layers.37.self_attn\n",
      "vision_model.encoder.layers.37.self_attn.dropout\n",
      "vision_model.encoder.layers.37.self_attn.qkv\n",
      "vision_model.encoder.layers.37.self_attn.projection\n",
      "vision_model.encoder.layers.37.layer_norm1\n",
      "vision_model.encoder.layers.37.mlp\n",
      "vision_model.encoder.layers.37.mlp.activation_fn\n",
      "vision_model.encoder.layers.37.mlp.fc1\n",
      "vision_model.encoder.layers.37.mlp.fc2\n",
      "vision_model.encoder.layers.37.layer_norm2\n",
      "vision_model.encoder.layers.38\n",
      "vision_model.encoder.layers.38.self_attn\n",
      "vision_model.encoder.layers.38.self_attn.dropout\n",
      "vision_model.encoder.layers.38.self_attn.qkv\n",
      "vision_model.encoder.layers.38.self_attn.projection\n",
      "vision_model.encoder.layers.38.layer_norm1\n",
      "vision_model.encoder.layers.38.mlp\n",
      "vision_model.encoder.layers.38.mlp.activation_fn\n",
      "vision_model.encoder.layers.38.mlp.fc1\n",
      "vision_model.encoder.layers.38.mlp.fc2\n",
      "vision_model.encoder.layers.38.layer_norm2\n",
      "vision_model.encoder.layers.39\n",
      "vision_model.encoder.layers.39.self_attn\n",
      "vision_model.encoder.layers.39.self_attn.dropout\n",
      "vision_model.encoder.layers.39.self_attn.qkv\n",
      "vision_model.encoder.layers.39.self_attn.projection\n",
      "vision_model.encoder.layers.39.layer_norm1\n",
      "vision_model.encoder.layers.39.mlp\n",
      "vision_model.encoder.layers.39.mlp.activation_fn\n",
      "vision_model.encoder.layers.39.mlp.fc1\n",
      "vision_model.encoder.layers.39.mlp.fc2\n",
      "vision_model.encoder.layers.39.layer_norm2\n",
      "vision_model.post_layernorm\n",
      "qformer\n",
      "qformer.layernorm\n",
      "qformer.dropout\n",
      "qformer.encoder\n",
      "qformer.encoder.layer\n",
      "qformer.encoder.layer.0\n",
      "qformer.encoder.layer.0.attention\n",
      "qformer.encoder.layer.0.attention.attention\n",
      "qformer.encoder.layer.0.attention.attention.query\n",
      "qformer.encoder.layer.0.attention.attention.key\n",
      "qformer.encoder.layer.0.attention.attention.value\n",
      "qformer.encoder.layer.0.attention.attention.dropout\n",
      "qformer.encoder.layer.0.attention.output\n",
      "qformer.encoder.layer.0.attention.output.dense\n",
      "qformer.encoder.layer.0.attention.output.LayerNorm\n",
      "qformer.encoder.layer.0.attention.output.dropout\n",
      "qformer.encoder.layer.0.crossattention\n",
      "qformer.encoder.layer.0.crossattention.attention\n",
      "qformer.encoder.layer.0.crossattention.attention.query\n",
      "qformer.encoder.layer.0.crossattention.attention.key\n",
      "qformer.encoder.layer.0.crossattention.attention.value\n",
      "qformer.encoder.layer.0.crossattention.attention.dropout\n",
      "qformer.encoder.layer.0.crossattention.output\n",
      "qformer.encoder.layer.0.crossattention.output.dense\n",
      "qformer.encoder.layer.0.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.0.crossattention.output.dropout\n",
      "qformer.encoder.layer.0.intermediate_query\n",
      "qformer.encoder.layer.0.intermediate_query.dense\n",
      "qformer.encoder.layer.0.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.0.output_query\n",
      "qformer.encoder.layer.0.output_query.dense\n",
      "qformer.encoder.layer.0.output_query.LayerNorm\n",
      "qformer.encoder.layer.0.output_query.dropout\n",
      "qformer.encoder.layer.1\n",
      "qformer.encoder.layer.1.attention\n",
      "qformer.encoder.layer.1.attention.attention\n",
      "qformer.encoder.layer.1.attention.attention.query\n",
      "qformer.encoder.layer.1.attention.attention.key\n",
      "qformer.encoder.layer.1.attention.attention.value\n",
      "qformer.encoder.layer.1.attention.attention.dropout\n",
      "qformer.encoder.layer.1.attention.output\n",
      "qformer.encoder.layer.1.attention.output.dense\n",
      "qformer.encoder.layer.1.attention.output.LayerNorm\n",
      "qformer.encoder.layer.1.attention.output.dropout\n",
      "qformer.encoder.layer.1.intermediate_query\n",
      "qformer.encoder.layer.1.intermediate_query.dense\n",
      "qformer.encoder.layer.1.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.1.output_query\n",
      "qformer.encoder.layer.1.output_query.dense\n",
      "qformer.encoder.layer.1.output_query.LayerNorm\n",
      "qformer.encoder.layer.1.output_query.dropout\n",
      "qformer.encoder.layer.2\n",
      "qformer.encoder.layer.2.attention\n",
      "qformer.encoder.layer.2.attention.attention\n",
      "qformer.encoder.layer.2.attention.attention.query\n",
      "qformer.encoder.layer.2.attention.attention.key\n",
      "qformer.encoder.layer.2.attention.attention.value\n",
      "qformer.encoder.layer.2.attention.attention.dropout\n",
      "qformer.encoder.layer.2.attention.output\n",
      "qformer.encoder.layer.2.attention.output.dense\n",
      "qformer.encoder.layer.2.attention.output.LayerNorm\n",
      "qformer.encoder.layer.2.attention.output.dropout\n",
      "qformer.encoder.layer.2.crossattention\n",
      "qformer.encoder.layer.2.crossattention.attention\n",
      "qformer.encoder.layer.2.crossattention.attention.query\n",
      "qformer.encoder.layer.2.crossattention.attention.key\n",
      "qformer.encoder.layer.2.crossattention.attention.value\n",
      "qformer.encoder.layer.2.crossattention.attention.dropout\n",
      "qformer.encoder.layer.2.crossattention.output\n",
      "qformer.encoder.layer.2.crossattention.output.dense\n",
      "qformer.encoder.layer.2.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.2.crossattention.output.dropout\n",
      "qformer.encoder.layer.2.intermediate_query\n",
      "qformer.encoder.layer.2.intermediate_query.dense\n",
      "qformer.encoder.layer.2.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.2.output_query\n",
      "qformer.encoder.layer.2.output_query.dense\n",
      "qformer.encoder.layer.2.output_query.LayerNorm\n",
      "qformer.encoder.layer.2.output_query.dropout\n",
      "qformer.encoder.layer.3\n",
      "qformer.encoder.layer.3.attention\n",
      "qformer.encoder.layer.3.attention.attention\n",
      "qformer.encoder.layer.3.attention.attention.query\n",
      "qformer.encoder.layer.3.attention.attention.key\n",
      "qformer.encoder.layer.3.attention.attention.value\n",
      "qformer.encoder.layer.3.attention.attention.dropout\n",
      "qformer.encoder.layer.3.attention.output\n",
      "qformer.encoder.layer.3.attention.output.dense\n",
      "qformer.encoder.layer.3.attention.output.LayerNorm\n",
      "qformer.encoder.layer.3.attention.output.dropout\n",
      "qformer.encoder.layer.3.intermediate_query\n",
      "qformer.encoder.layer.3.intermediate_query.dense\n",
      "qformer.encoder.layer.3.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.3.output_query\n",
      "qformer.encoder.layer.3.output_query.dense\n",
      "qformer.encoder.layer.3.output_query.LayerNorm\n",
      "qformer.encoder.layer.3.output_query.dropout\n",
      "qformer.encoder.layer.4\n",
      "qformer.encoder.layer.4.attention\n",
      "qformer.encoder.layer.4.attention.attention\n",
      "qformer.encoder.layer.4.attention.attention.query\n",
      "qformer.encoder.layer.4.attention.attention.key\n",
      "qformer.encoder.layer.4.attention.attention.value\n",
      "qformer.encoder.layer.4.attention.attention.dropout\n",
      "qformer.encoder.layer.4.attention.output\n",
      "qformer.encoder.layer.4.attention.output.dense\n",
      "qformer.encoder.layer.4.attention.output.LayerNorm\n",
      "qformer.encoder.layer.4.attention.output.dropout\n",
      "qformer.encoder.layer.4.crossattention\n",
      "qformer.encoder.layer.4.crossattention.attention\n",
      "qformer.encoder.layer.4.crossattention.attention.query\n",
      "qformer.encoder.layer.4.crossattention.attention.key\n",
      "qformer.encoder.layer.4.crossattention.attention.value\n",
      "qformer.encoder.layer.4.crossattention.attention.dropout\n",
      "qformer.encoder.layer.4.crossattention.output\n",
      "qformer.encoder.layer.4.crossattention.output.dense\n",
      "qformer.encoder.layer.4.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.4.crossattention.output.dropout\n",
      "qformer.encoder.layer.4.intermediate_query\n",
      "qformer.encoder.layer.4.intermediate_query.dense\n",
      "qformer.encoder.layer.4.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.4.output_query\n",
      "qformer.encoder.layer.4.output_query.dense\n",
      "qformer.encoder.layer.4.output_query.LayerNorm\n",
      "qformer.encoder.layer.4.output_query.dropout\n",
      "qformer.encoder.layer.5\n",
      "qformer.encoder.layer.5.attention\n",
      "qformer.encoder.layer.5.attention.attention\n",
      "qformer.encoder.layer.5.attention.attention.query\n",
      "qformer.encoder.layer.5.attention.attention.key\n",
      "qformer.encoder.layer.5.attention.attention.value\n",
      "qformer.encoder.layer.5.attention.attention.dropout\n",
      "qformer.encoder.layer.5.attention.output\n",
      "qformer.encoder.layer.5.attention.output.dense\n",
      "qformer.encoder.layer.5.attention.output.LayerNorm\n",
      "qformer.encoder.layer.5.attention.output.dropout\n",
      "qformer.encoder.layer.5.intermediate_query\n",
      "qformer.encoder.layer.5.intermediate_query.dense\n",
      "qformer.encoder.layer.5.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.5.output_query\n",
      "qformer.encoder.layer.5.output_query.dense\n",
      "qformer.encoder.layer.5.output_query.LayerNorm\n",
      "qformer.encoder.layer.5.output_query.dropout\n",
      "qformer.encoder.layer.6\n",
      "qformer.encoder.layer.6.attention\n",
      "qformer.encoder.layer.6.attention.attention\n",
      "qformer.encoder.layer.6.attention.attention.query\n",
      "qformer.encoder.layer.6.attention.attention.key\n",
      "qformer.encoder.layer.6.attention.attention.value\n",
      "qformer.encoder.layer.6.attention.attention.dropout\n",
      "qformer.encoder.layer.6.attention.output\n",
      "qformer.encoder.layer.6.attention.output.dense\n",
      "qformer.encoder.layer.6.attention.output.LayerNorm\n",
      "qformer.encoder.layer.6.attention.output.dropout\n",
      "qformer.encoder.layer.6.crossattention\n",
      "qformer.encoder.layer.6.crossattention.attention\n",
      "qformer.encoder.layer.6.crossattention.attention.query\n",
      "qformer.encoder.layer.6.crossattention.attention.key\n",
      "qformer.encoder.layer.6.crossattention.attention.value\n",
      "qformer.encoder.layer.6.crossattention.attention.dropout\n",
      "qformer.encoder.layer.6.crossattention.output\n",
      "qformer.encoder.layer.6.crossattention.output.dense\n",
      "qformer.encoder.layer.6.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.6.crossattention.output.dropout\n",
      "qformer.encoder.layer.6.intermediate_query\n",
      "qformer.encoder.layer.6.intermediate_query.dense\n",
      "qformer.encoder.layer.6.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.6.output_query\n",
      "qformer.encoder.layer.6.output_query.dense\n",
      "qformer.encoder.layer.6.output_query.LayerNorm\n",
      "qformer.encoder.layer.6.output_query.dropout\n",
      "qformer.encoder.layer.7\n",
      "qformer.encoder.layer.7.attention\n",
      "qformer.encoder.layer.7.attention.attention\n",
      "qformer.encoder.layer.7.attention.attention.query\n",
      "qformer.encoder.layer.7.attention.attention.key\n",
      "qformer.encoder.layer.7.attention.attention.value\n",
      "qformer.encoder.layer.7.attention.attention.dropout\n",
      "qformer.encoder.layer.7.attention.output\n",
      "qformer.encoder.layer.7.attention.output.dense\n",
      "qformer.encoder.layer.7.attention.output.LayerNorm\n",
      "qformer.encoder.layer.7.attention.output.dropout\n",
      "qformer.encoder.layer.7.intermediate_query\n",
      "qformer.encoder.layer.7.intermediate_query.dense\n",
      "qformer.encoder.layer.7.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.7.output_query\n",
      "qformer.encoder.layer.7.output_query.dense\n",
      "qformer.encoder.layer.7.output_query.LayerNorm\n",
      "qformer.encoder.layer.7.output_query.dropout\n",
      "qformer.encoder.layer.8\n",
      "qformer.encoder.layer.8.attention\n",
      "qformer.encoder.layer.8.attention.attention\n",
      "qformer.encoder.layer.8.attention.attention.query\n",
      "qformer.encoder.layer.8.attention.attention.key\n",
      "qformer.encoder.layer.8.attention.attention.value\n",
      "qformer.encoder.layer.8.attention.attention.dropout\n",
      "qformer.encoder.layer.8.attention.output\n",
      "qformer.encoder.layer.8.attention.output.dense\n",
      "qformer.encoder.layer.8.attention.output.LayerNorm\n",
      "qformer.encoder.layer.8.attention.output.dropout\n",
      "qformer.encoder.layer.8.crossattention\n",
      "qformer.encoder.layer.8.crossattention.attention\n",
      "qformer.encoder.layer.8.crossattention.attention.query\n",
      "qformer.encoder.layer.8.crossattention.attention.key\n",
      "qformer.encoder.layer.8.crossattention.attention.value\n",
      "qformer.encoder.layer.8.crossattention.attention.dropout\n",
      "qformer.encoder.layer.8.crossattention.output\n",
      "qformer.encoder.layer.8.crossattention.output.dense\n",
      "qformer.encoder.layer.8.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.8.crossattention.output.dropout\n",
      "qformer.encoder.layer.8.intermediate_query\n",
      "qformer.encoder.layer.8.intermediate_query.dense\n",
      "qformer.encoder.layer.8.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.8.output_query\n",
      "qformer.encoder.layer.8.output_query.dense\n",
      "qformer.encoder.layer.8.output_query.LayerNorm\n",
      "qformer.encoder.layer.8.output_query.dropout\n",
      "qformer.encoder.layer.9\n",
      "qformer.encoder.layer.9.attention\n",
      "qformer.encoder.layer.9.attention.attention\n",
      "qformer.encoder.layer.9.attention.attention.query\n",
      "qformer.encoder.layer.9.attention.attention.key\n",
      "qformer.encoder.layer.9.attention.attention.value\n",
      "qformer.encoder.layer.9.attention.attention.dropout\n",
      "qformer.encoder.layer.9.attention.output\n",
      "qformer.encoder.layer.9.attention.output.dense\n",
      "qformer.encoder.layer.9.attention.output.LayerNorm\n",
      "qformer.encoder.layer.9.attention.output.dropout\n",
      "qformer.encoder.layer.9.intermediate_query\n",
      "qformer.encoder.layer.9.intermediate_query.dense\n",
      "qformer.encoder.layer.9.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.9.output_query\n",
      "qformer.encoder.layer.9.output_query.dense\n",
      "qformer.encoder.layer.9.output_query.LayerNorm\n",
      "qformer.encoder.layer.9.output_query.dropout\n",
      "qformer.encoder.layer.10\n",
      "qformer.encoder.layer.10.attention\n",
      "qformer.encoder.layer.10.attention.attention\n",
      "qformer.encoder.layer.10.attention.attention.query\n",
      "qformer.encoder.layer.10.attention.attention.key\n",
      "qformer.encoder.layer.10.attention.attention.value\n",
      "qformer.encoder.layer.10.attention.attention.dropout\n",
      "qformer.encoder.layer.10.attention.output\n",
      "qformer.encoder.layer.10.attention.output.dense\n",
      "qformer.encoder.layer.10.attention.output.LayerNorm\n",
      "qformer.encoder.layer.10.attention.output.dropout\n",
      "qformer.encoder.layer.10.crossattention\n",
      "qformer.encoder.layer.10.crossattention.attention\n",
      "qformer.encoder.layer.10.crossattention.attention.query\n",
      "qformer.encoder.layer.10.crossattention.attention.key\n",
      "qformer.encoder.layer.10.crossattention.attention.value\n",
      "qformer.encoder.layer.10.crossattention.attention.dropout\n",
      "qformer.encoder.layer.10.crossattention.output\n",
      "qformer.encoder.layer.10.crossattention.output.dense\n",
      "qformer.encoder.layer.10.crossattention.output.LayerNorm\n",
      "qformer.encoder.layer.10.crossattention.output.dropout\n",
      "qformer.encoder.layer.10.intermediate_query\n",
      "qformer.encoder.layer.10.intermediate_query.dense\n",
      "qformer.encoder.layer.10.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.10.output_query\n",
      "qformer.encoder.layer.10.output_query.dense\n",
      "qformer.encoder.layer.10.output_query.LayerNorm\n",
      "qformer.encoder.layer.10.output_query.dropout\n",
      "qformer.encoder.layer.11\n",
      "qformer.encoder.layer.11.attention\n",
      "qformer.encoder.layer.11.attention.attention\n",
      "qformer.encoder.layer.11.attention.attention.query\n",
      "qformer.encoder.layer.11.attention.attention.key\n",
      "qformer.encoder.layer.11.attention.attention.value\n",
      "qformer.encoder.layer.11.attention.attention.dropout\n",
      "qformer.encoder.layer.11.attention.output\n",
      "qformer.encoder.layer.11.attention.output.dense\n",
      "qformer.encoder.layer.11.attention.output.LayerNorm\n",
      "qformer.encoder.layer.11.attention.output.dropout\n",
      "qformer.encoder.layer.11.intermediate_query\n",
      "qformer.encoder.layer.11.intermediate_query.dense\n",
      "qformer.encoder.layer.11.intermediate_query.intermediate_act_fn\n",
      "qformer.encoder.layer.11.output_query\n",
      "qformer.encoder.layer.11.output_query.dense\n",
      "qformer.encoder.layer.11.output_query.LayerNorm\n",
      "qformer.encoder.layer.11.output_query.dropout\n",
      "language_projection\n",
      "language_model\n",
      "language_model.model\n",
      "language_model.model.embed_tokens\n",
      "language_model.model.layers\n",
      "language_model.model.layers.0\n",
      "language_model.model.layers.0.self_attn\n",
      "language_model.model.layers.0.self_attn.q_proj\n",
      "language_model.model.layers.0.self_attn.k_proj\n",
      "language_model.model.layers.0.self_attn.v_proj\n",
      "language_model.model.layers.0.self_attn.o_proj\n",
      "language_model.model.layers.0.self_attn.rotary_emb\n",
      "language_model.model.layers.0.mlp\n",
      "language_model.model.layers.0.mlp.gate_proj\n",
      "language_model.model.layers.0.mlp.up_proj\n",
      "language_model.model.layers.0.mlp.down_proj\n",
      "language_model.model.layers.0.mlp.act_fn\n",
      "language_model.model.layers.0.input_layernorm\n",
      "language_model.model.layers.0.post_attention_layernorm\n",
      "language_model.model.layers.1\n",
      "language_model.model.layers.1.self_attn\n",
      "language_model.model.layers.1.self_attn.q_proj\n",
      "language_model.model.layers.1.self_attn.k_proj\n",
      "language_model.model.layers.1.self_attn.v_proj\n",
      "language_model.model.layers.1.self_attn.o_proj\n",
      "language_model.model.layers.1.self_attn.rotary_emb\n",
      "language_model.model.layers.1.mlp\n",
      "language_model.model.layers.1.mlp.gate_proj\n",
      "language_model.model.layers.1.mlp.up_proj\n",
      "language_model.model.layers.1.mlp.down_proj\n",
      "language_model.model.layers.1.mlp.act_fn\n",
      "language_model.model.layers.1.input_layernorm\n",
      "language_model.model.layers.1.post_attention_layernorm\n",
      "language_model.model.layers.2\n",
      "language_model.model.layers.2.self_attn\n",
      "language_model.model.layers.2.self_attn.q_proj\n",
      "language_model.model.layers.2.self_attn.k_proj\n",
      "language_model.model.layers.2.self_attn.v_proj\n",
      "language_model.model.layers.2.self_attn.o_proj\n",
      "language_model.model.layers.2.self_attn.rotary_emb\n",
      "language_model.model.layers.2.mlp\n",
      "language_model.model.layers.2.mlp.gate_proj\n",
      "language_model.model.layers.2.mlp.up_proj\n",
      "language_model.model.layers.2.mlp.down_proj\n",
      "language_model.model.layers.2.mlp.act_fn\n",
      "language_model.model.layers.2.input_layernorm\n",
      "language_model.model.layers.2.post_attention_layernorm\n",
      "language_model.model.layers.3\n",
      "language_model.model.layers.3.self_attn\n",
      "language_model.model.layers.3.self_attn.q_proj\n",
      "language_model.model.layers.3.self_attn.k_proj\n",
      "language_model.model.layers.3.self_attn.v_proj\n",
      "language_model.model.layers.3.self_attn.o_proj\n",
      "language_model.model.layers.3.self_attn.rotary_emb\n",
      "language_model.model.layers.3.mlp\n",
      "language_model.model.layers.3.mlp.gate_proj\n",
      "language_model.model.layers.3.mlp.up_proj\n",
      "language_model.model.layers.3.mlp.down_proj\n",
      "language_model.model.layers.3.mlp.act_fn\n",
      "language_model.model.layers.3.input_layernorm\n",
      "language_model.model.layers.3.post_attention_layernorm\n",
      "language_model.model.layers.4\n",
      "language_model.model.layers.4.self_attn\n",
      "language_model.model.layers.4.self_attn.q_proj\n",
      "language_model.model.layers.4.self_attn.k_proj\n",
      "language_model.model.layers.4.self_attn.v_proj\n",
      "language_model.model.layers.4.self_attn.o_proj\n",
      "language_model.model.layers.4.self_attn.rotary_emb\n",
      "language_model.model.layers.4.mlp\n",
      "language_model.model.layers.4.mlp.gate_proj\n",
      "language_model.model.layers.4.mlp.up_proj\n",
      "language_model.model.layers.4.mlp.down_proj\n",
      "language_model.model.layers.4.mlp.act_fn\n",
      "language_model.model.layers.4.input_layernorm\n",
      "language_model.model.layers.4.post_attention_layernorm\n",
      "language_model.model.layers.5\n",
      "language_model.model.layers.5.self_attn\n",
      "language_model.model.layers.5.self_attn.q_proj\n",
      "language_model.model.layers.5.self_attn.k_proj\n",
      "language_model.model.layers.5.self_attn.v_proj\n",
      "language_model.model.layers.5.self_attn.o_proj\n",
      "language_model.model.layers.5.self_attn.rotary_emb\n",
      "language_model.model.layers.5.mlp\n",
      "language_model.model.layers.5.mlp.gate_proj\n",
      "language_model.model.layers.5.mlp.up_proj\n",
      "language_model.model.layers.5.mlp.down_proj\n",
      "language_model.model.layers.5.mlp.act_fn\n",
      "language_model.model.layers.5.input_layernorm\n",
      "language_model.model.layers.5.post_attention_layernorm\n",
      "language_model.model.layers.6\n",
      "language_model.model.layers.6.self_attn\n",
      "language_model.model.layers.6.self_attn.q_proj\n",
      "language_model.model.layers.6.self_attn.k_proj\n",
      "language_model.model.layers.6.self_attn.v_proj\n",
      "language_model.model.layers.6.self_attn.o_proj\n",
      "language_model.model.layers.6.self_attn.rotary_emb\n",
      "language_model.model.layers.6.mlp\n",
      "language_model.model.layers.6.mlp.gate_proj\n",
      "language_model.model.layers.6.mlp.up_proj\n",
      "language_model.model.layers.6.mlp.down_proj\n",
      "language_model.model.layers.6.mlp.act_fn\n",
      "language_model.model.layers.6.input_layernorm\n",
      "language_model.model.layers.6.post_attention_layernorm\n",
      "language_model.model.layers.7\n",
      "language_model.model.layers.7.self_attn\n",
      "language_model.model.layers.7.self_attn.q_proj\n",
      "language_model.model.layers.7.self_attn.k_proj\n",
      "language_model.model.layers.7.self_attn.v_proj\n",
      "language_model.model.layers.7.self_attn.o_proj\n",
      "language_model.model.layers.7.self_attn.rotary_emb\n",
      "language_model.model.layers.7.mlp\n",
      "language_model.model.layers.7.mlp.gate_proj\n",
      "language_model.model.layers.7.mlp.up_proj\n",
      "language_model.model.layers.7.mlp.down_proj\n",
      "language_model.model.layers.7.mlp.act_fn\n",
      "language_model.model.layers.7.input_layernorm\n",
      "language_model.model.layers.7.post_attention_layernorm\n",
      "language_model.model.layers.8\n",
      "language_model.model.layers.8.self_attn\n",
      "language_model.model.layers.8.self_attn.q_proj\n",
      "language_model.model.layers.8.self_attn.k_proj\n",
      "language_model.model.layers.8.self_attn.v_proj\n",
      "language_model.model.layers.8.self_attn.o_proj\n",
      "language_model.model.layers.8.self_attn.rotary_emb\n",
      "language_model.model.layers.8.mlp\n",
      "language_model.model.layers.8.mlp.gate_proj\n",
      "language_model.model.layers.8.mlp.up_proj\n",
      "language_model.model.layers.8.mlp.down_proj\n",
      "language_model.model.layers.8.mlp.act_fn\n",
      "language_model.model.layers.8.input_layernorm\n",
      "language_model.model.layers.8.post_attention_layernorm\n",
      "language_model.model.layers.9\n",
      "language_model.model.layers.9.self_attn\n",
      "language_model.model.layers.9.self_attn.q_proj\n",
      "language_model.model.layers.9.self_attn.k_proj\n",
      "language_model.model.layers.9.self_attn.v_proj\n",
      "language_model.model.layers.9.self_attn.o_proj\n",
      "language_model.model.layers.9.self_attn.rotary_emb\n",
      "language_model.model.layers.9.mlp\n",
      "language_model.model.layers.9.mlp.gate_proj\n",
      "language_model.model.layers.9.mlp.up_proj\n",
      "language_model.model.layers.9.mlp.down_proj\n",
      "language_model.model.layers.9.mlp.act_fn\n",
      "language_model.model.layers.9.input_layernorm\n",
      "language_model.model.layers.9.post_attention_layernorm\n",
      "language_model.model.layers.10\n",
      "language_model.model.layers.10.self_attn\n",
      "language_model.model.layers.10.self_attn.q_proj\n",
      "language_model.model.layers.10.self_attn.k_proj\n",
      "language_model.model.layers.10.self_attn.v_proj\n",
      "language_model.model.layers.10.self_attn.o_proj\n",
      "language_model.model.layers.10.self_attn.rotary_emb\n",
      "language_model.model.layers.10.mlp\n",
      "language_model.model.layers.10.mlp.gate_proj\n",
      "language_model.model.layers.10.mlp.up_proj\n",
      "language_model.model.layers.10.mlp.down_proj\n",
      "language_model.model.layers.10.mlp.act_fn\n",
      "language_model.model.layers.10.input_layernorm\n",
      "language_model.model.layers.10.post_attention_layernorm\n",
      "language_model.model.layers.11\n",
      "language_model.model.layers.11.self_attn\n",
      "language_model.model.layers.11.self_attn.q_proj\n",
      "language_model.model.layers.11.self_attn.k_proj\n",
      "language_model.model.layers.11.self_attn.v_proj\n",
      "language_model.model.layers.11.self_attn.o_proj\n",
      "language_model.model.layers.11.self_attn.rotary_emb\n",
      "language_model.model.layers.11.mlp\n",
      "language_model.model.layers.11.mlp.gate_proj\n",
      "language_model.model.layers.11.mlp.up_proj\n",
      "language_model.model.layers.11.mlp.down_proj\n",
      "language_model.model.layers.11.mlp.act_fn\n",
      "language_model.model.layers.11.input_layernorm\n",
      "language_model.model.layers.11.post_attention_layernorm\n",
      "language_model.model.layers.12\n",
      "language_model.model.layers.12.self_attn\n",
      "language_model.model.layers.12.self_attn.q_proj\n",
      "language_model.model.layers.12.self_attn.k_proj\n",
      "language_model.model.layers.12.self_attn.v_proj\n",
      "language_model.model.layers.12.self_attn.o_proj\n",
      "language_model.model.layers.12.self_attn.rotary_emb\n",
      "language_model.model.layers.12.mlp\n",
      "language_model.model.layers.12.mlp.gate_proj\n",
      "language_model.model.layers.12.mlp.up_proj\n",
      "language_model.model.layers.12.mlp.down_proj\n",
      "language_model.model.layers.12.mlp.act_fn\n",
      "language_model.model.layers.12.input_layernorm\n",
      "language_model.model.layers.12.post_attention_layernorm\n",
      "language_model.model.layers.13\n",
      "language_model.model.layers.13.self_attn\n",
      "language_model.model.layers.13.self_attn.q_proj\n",
      "language_model.model.layers.13.self_attn.k_proj\n",
      "language_model.model.layers.13.self_attn.v_proj\n",
      "language_model.model.layers.13.self_attn.o_proj\n",
      "language_model.model.layers.13.self_attn.rotary_emb\n",
      "language_model.model.layers.13.mlp\n",
      "language_model.model.layers.13.mlp.gate_proj\n",
      "language_model.model.layers.13.mlp.up_proj\n",
      "language_model.model.layers.13.mlp.down_proj\n",
      "language_model.model.layers.13.mlp.act_fn\n",
      "language_model.model.layers.13.input_layernorm\n",
      "language_model.model.layers.13.post_attention_layernorm\n",
      "language_model.model.layers.14\n",
      "language_model.model.layers.14.self_attn\n",
      "language_model.model.layers.14.self_attn.q_proj\n",
      "language_model.model.layers.14.self_attn.k_proj\n",
      "language_model.model.layers.14.self_attn.v_proj\n",
      "language_model.model.layers.14.self_attn.o_proj\n",
      "language_model.model.layers.14.self_attn.rotary_emb\n",
      "language_model.model.layers.14.mlp\n",
      "language_model.model.layers.14.mlp.gate_proj\n",
      "language_model.model.layers.14.mlp.up_proj\n",
      "language_model.model.layers.14.mlp.down_proj\n",
      "language_model.model.layers.14.mlp.act_fn\n",
      "language_model.model.layers.14.input_layernorm\n",
      "language_model.model.layers.14.post_attention_layernorm\n",
      "language_model.model.layers.15\n",
      "language_model.model.layers.15.self_attn\n",
      "language_model.model.layers.15.self_attn.q_proj\n",
      "language_model.model.layers.15.self_attn.k_proj\n",
      "language_model.model.layers.15.self_attn.v_proj\n",
      "language_model.model.layers.15.self_attn.o_proj\n",
      "language_model.model.layers.15.self_attn.rotary_emb\n",
      "language_model.model.layers.15.mlp\n",
      "language_model.model.layers.15.mlp.gate_proj\n",
      "language_model.model.layers.15.mlp.up_proj\n",
      "language_model.model.layers.15.mlp.down_proj\n",
      "language_model.model.layers.15.mlp.act_fn\n",
      "language_model.model.layers.15.input_layernorm\n",
      "language_model.model.layers.15.post_attention_layernorm\n",
      "language_model.model.layers.16\n",
      "language_model.model.layers.16.self_attn\n",
      "language_model.model.layers.16.self_attn.q_proj\n",
      "language_model.model.layers.16.self_attn.k_proj\n",
      "language_model.model.layers.16.self_attn.v_proj\n",
      "language_model.model.layers.16.self_attn.o_proj\n",
      "language_model.model.layers.16.self_attn.rotary_emb\n",
      "language_model.model.layers.16.mlp\n",
      "language_model.model.layers.16.mlp.gate_proj\n",
      "language_model.model.layers.16.mlp.up_proj\n",
      "language_model.model.layers.16.mlp.down_proj\n",
      "language_model.model.layers.16.mlp.act_fn\n",
      "language_model.model.layers.16.input_layernorm\n",
      "language_model.model.layers.16.post_attention_layernorm\n",
      "language_model.model.layers.17\n",
      "language_model.model.layers.17.self_attn\n",
      "language_model.model.layers.17.self_attn.q_proj\n",
      "language_model.model.layers.17.self_attn.k_proj\n",
      "language_model.model.layers.17.self_attn.v_proj\n",
      "language_model.model.layers.17.self_attn.o_proj\n",
      "language_model.model.layers.17.self_attn.rotary_emb\n",
      "language_model.model.layers.17.mlp\n",
      "language_model.model.layers.17.mlp.gate_proj\n",
      "language_model.model.layers.17.mlp.up_proj\n",
      "language_model.model.layers.17.mlp.down_proj\n",
      "language_model.model.layers.17.mlp.act_fn\n",
      "language_model.model.layers.17.input_layernorm\n",
      "language_model.model.layers.17.post_attention_layernorm\n",
      "language_model.model.layers.18\n",
      "language_model.model.layers.18.self_attn\n",
      "language_model.model.layers.18.self_attn.q_proj\n",
      "language_model.model.layers.18.self_attn.k_proj\n",
      "language_model.model.layers.18.self_attn.v_proj\n",
      "language_model.model.layers.18.self_attn.o_proj\n",
      "language_model.model.layers.18.self_attn.rotary_emb\n",
      "language_model.model.layers.18.mlp\n",
      "language_model.model.layers.18.mlp.gate_proj\n",
      "language_model.model.layers.18.mlp.up_proj\n",
      "language_model.model.layers.18.mlp.down_proj\n",
      "language_model.model.layers.18.mlp.act_fn\n",
      "language_model.model.layers.18.input_layernorm\n",
      "language_model.model.layers.18.post_attention_layernorm\n",
      "language_model.model.layers.19\n",
      "language_model.model.layers.19.self_attn\n",
      "language_model.model.layers.19.self_attn.q_proj\n",
      "language_model.model.layers.19.self_attn.k_proj\n",
      "language_model.model.layers.19.self_attn.v_proj\n",
      "language_model.model.layers.19.self_attn.o_proj\n",
      "language_model.model.layers.19.self_attn.rotary_emb\n",
      "language_model.model.layers.19.mlp\n",
      "language_model.model.layers.19.mlp.gate_proj\n",
      "language_model.model.layers.19.mlp.up_proj\n",
      "language_model.model.layers.19.mlp.down_proj\n",
      "language_model.model.layers.19.mlp.act_fn\n",
      "language_model.model.layers.19.input_layernorm\n",
      "language_model.model.layers.19.post_attention_layernorm\n",
      "language_model.model.layers.20\n",
      "language_model.model.layers.20.self_attn\n",
      "language_model.model.layers.20.self_attn.q_proj\n",
      "language_model.model.layers.20.self_attn.k_proj\n",
      "language_model.model.layers.20.self_attn.v_proj\n",
      "language_model.model.layers.20.self_attn.o_proj\n",
      "language_model.model.layers.20.self_attn.rotary_emb\n",
      "language_model.model.layers.20.mlp\n",
      "language_model.model.layers.20.mlp.gate_proj\n",
      "language_model.model.layers.20.mlp.up_proj\n",
      "language_model.model.layers.20.mlp.down_proj\n",
      "language_model.model.layers.20.mlp.act_fn\n",
      "language_model.model.layers.20.input_layernorm\n",
      "language_model.model.layers.20.post_attention_layernorm\n",
      "language_model.model.layers.21\n",
      "language_model.model.layers.21.self_attn\n",
      "language_model.model.layers.21.self_attn.q_proj\n",
      "language_model.model.layers.21.self_attn.k_proj\n",
      "language_model.model.layers.21.self_attn.v_proj\n",
      "language_model.model.layers.21.self_attn.o_proj\n",
      "language_model.model.layers.21.self_attn.rotary_emb\n",
      "language_model.model.layers.21.mlp\n",
      "language_model.model.layers.21.mlp.gate_proj\n",
      "language_model.model.layers.21.mlp.up_proj\n",
      "language_model.model.layers.21.mlp.down_proj\n",
      "language_model.model.layers.21.mlp.act_fn\n",
      "language_model.model.layers.21.input_layernorm\n",
      "language_model.model.layers.21.post_attention_layernorm\n",
      "language_model.model.layers.22\n",
      "language_model.model.layers.22.self_attn\n",
      "language_model.model.layers.22.self_attn.q_proj\n",
      "language_model.model.layers.22.self_attn.k_proj\n",
      "language_model.model.layers.22.self_attn.v_proj\n",
      "language_model.model.layers.22.self_attn.o_proj\n",
      "language_model.model.layers.22.self_attn.rotary_emb\n",
      "language_model.model.layers.22.mlp\n",
      "language_model.model.layers.22.mlp.gate_proj\n",
      "language_model.model.layers.22.mlp.up_proj\n",
      "language_model.model.layers.22.mlp.down_proj\n",
      "language_model.model.layers.22.mlp.act_fn\n",
      "language_model.model.layers.22.input_layernorm\n",
      "language_model.model.layers.22.post_attention_layernorm\n",
      "language_model.model.layers.23\n",
      "language_model.model.layers.23.self_attn\n",
      "language_model.model.layers.23.self_attn.q_proj\n",
      "language_model.model.layers.23.self_attn.k_proj\n",
      "language_model.model.layers.23.self_attn.v_proj\n",
      "language_model.model.layers.23.self_attn.o_proj\n",
      "language_model.model.layers.23.self_attn.rotary_emb\n",
      "language_model.model.layers.23.mlp\n",
      "language_model.model.layers.23.mlp.gate_proj\n",
      "language_model.model.layers.23.mlp.up_proj\n",
      "language_model.model.layers.23.mlp.down_proj\n",
      "language_model.model.layers.23.mlp.act_fn\n",
      "language_model.model.layers.23.input_layernorm\n",
      "language_model.model.layers.23.post_attention_layernorm\n",
      "language_model.model.layers.24\n",
      "language_model.model.layers.24.self_attn\n",
      "language_model.model.layers.24.self_attn.q_proj\n",
      "language_model.model.layers.24.self_attn.k_proj\n",
      "language_model.model.layers.24.self_attn.v_proj\n",
      "language_model.model.layers.24.self_attn.o_proj\n",
      "language_model.model.layers.24.self_attn.rotary_emb\n",
      "language_model.model.layers.24.mlp\n",
      "language_model.model.layers.24.mlp.gate_proj\n",
      "language_model.model.layers.24.mlp.up_proj\n",
      "language_model.model.layers.24.mlp.down_proj\n",
      "language_model.model.layers.24.mlp.act_fn\n",
      "language_model.model.layers.24.input_layernorm\n",
      "language_model.model.layers.24.post_attention_layernorm\n",
      "language_model.model.layers.25\n",
      "language_model.model.layers.25.self_attn\n",
      "language_model.model.layers.25.self_attn.q_proj\n",
      "language_model.model.layers.25.self_attn.k_proj\n",
      "language_model.model.layers.25.self_attn.v_proj\n",
      "language_model.model.layers.25.self_attn.o_proj\n",
      "language_model.model.layers.25.self_attn.rotary_emb\n",
      "language_model.model.layers.25.mlp\n",
      "language_model.model.layers.25.mlp.gate_proj\n",
      "language_model.model.layers.25.mlp.up_proj\n",
      "language_model.model.layers.25.mlp.down_proj\n",
      "language_model.model.layers.25.mlp.act_fn\n",
      "language_model.model.layers.25.input_layernorm\n",
      "language_model.model.layers.25.post_attention_layernorm\n",
      "language_model.model.layers.26\n",
      "language_model.model.layers.26.self_attn\n",
      "language_model.model.layers.26.self_attn.q_proj\n",
      "language_model.model.layers.26.self_attn.k_proj\n",
      "language_model.model.layers.26.self_attn.v_proj\n",
      "language_model.model.layers.26.self_attn.o_proj\n",
      "language_model.model.layers.26.self_attn.rotary_emb\n",
      "language_model.model.layers.26.mlp\n",
      "language_model.model.layers.26.mlp.gate_proj\n",
      "language_model.model.layers.26.mlp.up_proj\n",
      "language_model.model.layers.26.mlp.down_proj\n",
      "language_model.model.layers.26.mlp.act_fn\n",
      "language_model.model.layers.26.input_layernorm\n",
      "language_model.model.layers.26.post_attention_layernorm\n",
      "language_model.model.layers.27\n",
      "language_model.model.layers.27.self_attn\n",
      "language_model.model.layers.27.self_attn.q_proj\n",
      "language_model.model.layers.27.self_attn.k_proj\n",
      "language_model.model.layers.27.self_attn.v_proj\n",
      "language_model.model.layers.27.self_attn.o_proj\n",
      "language_model.model.layers.27.self_attn.rotary_emb\n",
      "language_model.model.layers.27.mlp\n",
      "language_model.model.layers.27.mlp.gate_proj\n",
      "language_model.model.layers.27.mlp.up_proj\n",
      "language_model.model.layers.27.mlp.down_proj\n",
      "language_model.model.layers.27.mlp.act_fn\n",
      "language_model.model.layers.27.input_layernorm\n",
      "language_model.model.layers.27.post_attention_layernorm\n",
      "language_model.model.layers.28\n",
      "language_model.model.layers.28.self_attn\n",
      "language_model.model.layers.28.self_attn.q_proj\n",
      "language_model.model.layers.28.self_attn.k_proj\n",
      "language_model.model.layers.28.self_attn.v_proj\n",
      "language_model.model.layers.28.self_attn.o_proj\n",
      "language_model.model.layers.28.self_attn.rotary_emb\n",
      "language_model.model.layers.28.mlp\n",
      "language_model.model.layers.28.mlp.gate_proj\n",
      "language_model.model.layers.28.mlp.up_proj\n",
      "language_model.model.layers.28.mlp.down_proj\n",
      "language_model.model.layers.28.mlp.act_fn\n",
      "language_model.model.layers.28.input_layernorm\n",
      "language_model.model.layers.28.post_attention_layernorm\n",
      "language_model.model.layers.29\n",
      "language_model.model.layers.29.self_attn\n",
      "language_model.model.layers.29.self_attn.q_proj\n",
      "language_model.model.layers.29.self_attn.k_proj\n",
      "language_model.model.layers.29.self_attn.v_proj\n",
      "language_model.model.layers.29.self_attn.o_proj\n",
      "language_model.model.layers.29.self_attn.rotary_emb\n",
      "language_model.model.layers.29.mlp\n",
      "language_model.model.layers.29.mlp.gate_proj\n",
      "language_model.model.layers.29.mlp.up_proj\n",
      "language_model.model.layers.29.mlp.down_proj\n",
      "language_model.model.layers.29.mlp.act_fn\n",
      "language_model.model.layers.29.input_layernorm\n",
      "language_model.model.layers.29.post_attention_layernorm\n",
      "language_model.model.layers.30\n",
      "language_model.model.layers.30.self_attn\n",
      "language_model.model.layers.30.self_attn.q_proj\n",
      "language_model.model.layers.30.self_attn.k_proj\n",
      "language_model.model.layers.30.self_attn.v_proj\n",
      "language_model.model.layers.30.self_attn.o_proj\n",
      "language_model.model.layers.30.self_attn.rotary_emb\n",
      "language_model.model.layers.30.mlp\n",
      "language_model.model.layers.30.mlp.gate_proj\n",
      "language_model.model.layers.30.mlp.up_proj\n",
      "language_model.model.layers.30.mlp.down_proj\n",
      "language_model.model.layers.30.mlp.act_fn\n",
      "language_model.model.layers.30.input_layernorm\n",
      "language_model.model.layers.30.post_attention_layernorm\n",
      "language_model.model.layers.31\n",
      "language_model.model.layers.31.self_attn\n",
      "language_model.model.layers.31.self_attn.q_proj\n",
      "language_model.model.layers.31.self_attn.k_proj\n",
      "language_model.model.layers.31.self_attn.v_proj\n",
      "language_model.model.layers.31.self_attn.o_proj\n",
      "language_model.model.layers.31.self_attn.rotary_emb\n",
      "language_model.model.layers.31.mlp\n",
      "language_model.model.layers.31.mlp.gate_proj\n",
      "language_model.model.layers.31.mlp.up_proj\n",
      "language_model.model.layers.31.mlp.down_proj\n",
      "language_model.model.layers.31.mlp.act_fn\n",
      "language_model.model.layers.31.input_layernorm\n",
      "language_model.model.layers.31.post_attention_layernorm\n",
      "language_model.model.norm\n",
      "language_model.lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/xl_vlm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:41<00:00, 14.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "# Step 1: Setup constants\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16\n",
    "cache_dir = \"/netscratch/kadir/xl-vlms/cache/hub\"  # Your custom cache directory\n",
    "\n",
    "# Step 2: Load Processor and Model with cache_dir\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"StanfordAIMI/CheXagent-8b\", \n",
    "    cache_dir=cache_dir, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "generation_config = GenerationConfig.from_pretrained(\n",
    "    \"StanfordAIMI/CheXagent-8b\", \n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"StanfordAIMI/CheXagent-8b\", \n",
    "    cache_dir=cache_dir, \n",
    "    torch_dtype=dtype, \n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXagentForConditionalGeneration(\n",
      "  (vision_model): CheXagentVisionModel(\n",
      "    (embeddings): CheXagentVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (encoder): CheXagentEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-39): 40 x CheXagentEncoderLayer(\n",
      "          (self_attn): CheXagentAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
      "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): CheXagentMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
      "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (qformer): CheXagentQFormerModel(\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (encoder): CheXagentQFormerEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (language_projection): Linear(in_features=768, out_features=4096, bias=True)\n",
      "  (language_model): MistralForCausalLM(\n",
      "    (model): MistralModel(\n",
      "      (embed_tokens): Embedding(32000, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x MistralDecoderLayer(\n",
      "          (self_attn): MistralSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): MistralRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): MistralMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shape is: torch.Size([1, 1, 3, 448, 448])\n",
      "The language shape is: torch.Size([1, 25])\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 5\n",
      "}\n",
      "\n",
      "Generated Response:\n",
      "The hilar contours are normal.\n",
      "\n",
      "Layer Information (Name, Weight Shapes, Output Shape):\n",
      "\n",
      "Layer Name                                                   Weight Shapes                            Output Shape\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "vision_model.embeddings.patch_embedding                      [[1408, 3, 14, 14], [1408]]              torch.Size([1, 1408, 32, 32])\n",
      "vision_model.encoder.layers.39.mlp.fc2                       [[1408, 6144], [1408]]                   torch.Size([1, 1025, 1408])\n",
      "vision_model.post_layernorm                                  [[1408], [1408]]                         torch.Size([1, 1408])\n",
      "qformer.encoder.layer.11.output_query.dense                  [[768, 3072], [768]]                     torch.Size([1, 128, 768])\n",
      "language_projection                                          [[4096, 768], [4096]]                    torch.Size([1, 128, 4096])\n",
      "language_model.model.embed_tokens                            [[32000, 4096]]                          torch.Size([5, 1, 4096])\n",
      "language_model.model.layers.31                               [[4096, 4096], [1024, 4096], [1024, 4096], [4096, 4096], [14336, 4096], [14336, 4096], [4096, 14336], [4096], [4096]] Non-Tensor Output\n",
      "language_model.lm_head                                       [[32000, 4096]]                          torch.Size([5, 1, 32000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Load and prepare the image\n",
    "image_path = \"/home/kadir/xl-vlms/playground/images/synpic32933.jpg\"\n",
    "images = [Image.open(image_path).convert(\"RGB\")]\n",
    "\n",
    "# Step 2: Define the prompt and prepare inputs\n",
    "prompt = f'Describe \"Airway\" in 10 sentaance'\n",
    "inputs = processor(\n",
    "    images=images, \n",
    "    text=f\" USER: <s>{prompt} ASSISTANT: <s>\", \n",
    "    return_tensors=\"pt\"\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "# Show the input shapes\n",
    "print(f\"The image shape is: {inputs['pixel_values'].shape}\")\n",
    "print(f\"The language shape is: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Step 3: Initialize hooks and storage for layer output shapes and weights\n",
    "output_shapes = {}  # Dictionary to store layer output shapes\n",
    "weights = {}        # Dictionary to store weight shapes and sizes\n",
    "\n",
    "# Hook function to capture output shapes during the forward pass\n",
    "def hook_fn(module, input, output):\n",
    "    # Record the output shape of the module\n",
    "    output_shapes[module] = output.shape if isinstance(output, torch.Tensor) else \"Non-Tensor Output\"\n",
    "\n",
    "# Step 4: Register hooks and collect layer weight information\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    # Register hook for each layer to capture the output shape\n",
    "    hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Collect the weight shapes for each layer\n",
    "    layer_weights = []\n",
    "    for param in module.parameters():\n",
    "        layer_weights.append(list(param.shape))  # Record parameter shapes\n",
    "    weights[name] = layer_weights if layer_weights else \"No Weights\"\n",
    "\n",
    "# Step 5: Perform a forward pass with the model and generate the response\n",
    "print(generation_config)\n",
    "#generation_config.num_beams =1\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "# Decode and print the generated response\n",
    "response = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(f\"Generated Response:\\n{response}\")\n",
    "\n",
    "# Step 6: Print the layer information (name, weight shapes, and output shape)\n",
    "print(\"\\nLayer Information (Name, Weight Shapes, Output Shape):\\n\")\n",
    "print(f\"{'Layer Name':<60} {'Weight Shapes':<40} {'Output Shape'}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "# Filter and print only the relevant layers for analysis\n",
    "layers_of_interest = [\"vision_model.embeddings.patch_embedding\",\"vision_model.encoder.layers.39.mlp.fc2\", \"vision_model.post_layernorm\", \"language_model.model.embed_tokens\", \"qformer.encoder.layer.11.output_query.dense\", \"language_projection\",  \"language_model.model.embed_tokens\", \"language_model.model.layers.31\",\"language_model.lm_head\"]\n",
    "for module, shape in output_shapes.items():\n",
    "    # Get the layer name by searching in the model's named modules\n",
    "    layer_name = next((name for name, m in model.named_modules() if m == module), \"Unknown\")\n",
    "    \n",
    "    # If the layer is one of the layers of interest, print its info\n",
    "    if layer_name in layers_of_interest:\n",
    "        weight_shapes = str(weights[layer_name]) if weights[layer_name] != \"No Weights\" else \"No Weights\"\n",
    "        print(f\"{layer_name:<60} {weight_shapes:<40} {shape}\")\n",
    "\n",
    "# Clean up hooks to avoid memory leaks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qformer outptut = (128, 768)\n",
    "language_projecton = (128, 4096)\n",
    "text_embedding = (numbe_of_tokens, 4096)\n",
    "inputs_embeds = torch.cat([input_vis, inputs_lang], dim=1) shape (batch,numbe_of_tokens+128, 4096 )\n",
    "language_model.model.layers.31/language_model.model.norm = [1, 128+num_token, 4096]\n",
    "\n",
    "output = [number of ouput token]\n",
    "# step 5: conditioned on the images and/or prompts\n",
    "        if pixel_values is not None:\n",
    "            inputs_embeds = torch.cat([input_vis, inputs_lang], dim=1)\n",
    "            attention_mask = torch.cat([vis_atts, lang_atts], dim=1)\n",
    "        else:\n",
    "            inputs_embeds = inputs_lang\n",
    "            attention_mask = lang_atts\n",
    "\n",
    "        outputs = self.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            **generate_kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheXagentForConditionalGeneration(\n",
      "  (vision_model): CheXagentVisionModel(\n",
      "    (embeddings): CheXagentVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (encoder): CheXagentEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-39): 40 x CheXagentEncoderLayer(\n",
      "          (self_attn): CheXagentAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
      "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): CheXagentMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
      "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (qformer): CheXagentQFormerModel(\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (encoder): CheXagentQFormerEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): CheXagentQFormerLayer(\n",
      "          (attention): CheXagentQFormerAttention(\n",
      "            (attention): CheXagentQFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CheXagentQFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): CheXagentQFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): CheXagentQFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (language_projection): Linear(in_features=768, out_features=4096, bias=True)\n",
      "  (language_model): MistralForCausalLM(\n",
      "    (model): MistralModel(\n",
      "      (embed_tokens): Embedding(32000, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x MistralDecoderLayer(\n",
      "          (self_attn): MistralSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): MistralRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): MistralMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/kadir/xl-vlms/playground/images/synpic32933.jpg\"\n",
    "images = [Image.open(image_path).convert(\"RGB\")]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/StanfordAIMI/CheXagent-8b/4934e91451945c8218c267aae9c34929a7677829/processing_chexagent.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(pixel_values) for pixel_values in encoding_image_processor[\"pixel_values\"]]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = f'Describe \"Airway\", '\n",
    "inputs = processor(images=images, text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\").to(device=device, dtype=dtype)\n",
    "model = model.to(device)\n",
    "output = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "response = processor.tokenizer.decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hilar contours are normal.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "from models.image_text_model import ImageTextModel\n",
    "import logging\n",
    "\n",
    "__all__ = [\"Molmo\"]\n",
    "\n",
    "\n",
    "class Molmo(ImageTextModel):\n",
    "\n",
    "    def set_model(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        logging.debug(\"This is a debug message (useful for detailed troubleshooting).\")\n",
    "        self.model_ = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            local_files_only=self.local_files_only,\n",
    "        )\n",
    "\n",
    "    def get_model(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_language_model(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self.model_.model.transformer\n",
    "\n",
    "    def get_lm_head(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self.model_.model.transformer.ff_out\n",
    "\n",
    "    def set_processor(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.processor_ = AutoProcessor.from_pretrained(\n",
    "            self.processor_name,\n",
    "            local_files_only=self.local_files_only,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        self.tokenizer_ = self.processor_.tokenizer\n",
    "\n",
    "    def set_preprocessor(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.preprocessor_ = self.preprocess_input\n",
    "\n",
    "    def get_conversation_template(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        response: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        conversation = instruction\n",
    "        if response:\n",
    "            conversation += f\" Answer: {response}\"\n",
    "        return conversation\n",
    "\n",
    "    def preprocess_input(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        image_file: str = None,\n",
    "        response: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        text = self.get_conversation_template(\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "            image_file=image_file,\n",
    "        )\n",
    "\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        inputs = self.processor_.process(\n",
    "            text=text,\n",
    "            images=[image],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def preprocessor(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        image_file: str = \"\",\n",
    "        response: str = \"\",\n",
    "        generation_mode: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        preprocessor = self.get_preprocessor()\n",
    "        inputs = preprocessor(\n",
    "            instruction=instruction,\n",
    "            image_file=image_file,\n",
    "            response=response,\n",
    "            generation_mode=generation_mode,\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        max_new_tokens: int = 200,\n",
    "        do_sample: bool = False,\n",
    "        **inputs: Dict[str, Any],\n",
    "    ):\n",
    "        inputs = {k: v.unsqueeze(0).to(self.model_.device) for k, v in inputs.items()}\n",
    "        device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with torch.autocast(\n",
    "            device_type=device_type, enabled=True, dtype=self.model_.dtype\n",
    "        ):\n",
    "            output = self.model_.generate_from_batch(\n",
    "                inputs,\n",
    "                GenerationConfig(\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    stop_strings=\"<|endoftext|>\",\n",
    "                    do_sample=do_sample,\n",
    "                ),\n",
    "                tokenizer=self.tokenizer_,\n",
    "            )\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "from models.image_text_model import ImageTextModel\n",
    "\n",
    "__all__ = [\"ChexAgent\"]\n",
    "\n",
    "\n",
    "class ChexAgent(ImageTextModel):\n",
    "\n",
    "    def set_model(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.model_ = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name_or_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            cache_dir ='/netscratch/kadir/xl-vlms/cache/hub',\n",
    "            local_files_only=self.local_files_only,\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_language_model(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self.model_.model\n",
    "\n",
    "    def get_lm_head(\n",
    "        self,\n",
    "    ) -> Callable:\n",
    "\n",
    "        return self.model_.lm_head\n",
    "\n",
    "    def set_processor(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.processor_ = AutoProcessor.from_pretrained(\n",
    "            self.processor_name,\n",
    "            local_files_only=self.local_files_only,\n",
    "        )\n",
    "        self.tokenizer_ = self.processor_.tokenizer\n",
    "\n",
    "    def set_preprocessor(\n",
    "        self,\n",
    "    ) -> None:\n",
    "\n",
    "        self.preprocessor_ = self.preprocess_input\n",
    "\n",
    "    def get_conversation_round(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        response: str = \"\",\n",
    "        image_file: str = \"\",\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": image_file,\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": instruction},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        if response:\n",
    "            conversation.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": response},\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "\n",
    "        return conversation\n",
    "\n",
    "    def get_conversation_template(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        response: str = \"\",\n",
    "        image_file: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        conversation = self.get_conversation_round(\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "            image_file=image_file,\n",
    "        )\n",
    "        return conversation\n",
    "\n",
    "    def preprocess_text(\n",
    "        self,\n",
    "        conversation,\n",
    "        generation_mode: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "\n",
    "        prompt = self.processor_.apply_chat_template(\n",
    "            conversation,\n",
    "            add_generation_prompt=generation_mode,\n",
    "            tokenize=False,\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def preprocess_images(\n",
    "        self,\n",
    "        conversation,\n",
    "        **kwargs: Any,\n",
    "    ) -> List:\n",
    "\n",
    "        image_inputs, video_inputs = process_vision_info(conversation)\n",
    "\n",
    "        return image_inputs, video_inputs\n",
    "\n",
    "    def preprocess_input(\n",
    "        self,\n",
    "        instruction: str = \"What are these?\",\n",
    "        image_file: str = None,\n",
    "        response: str = \"\",\n",
    "        generation_mode: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        conversation = self.get_conversation_template(\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "            image_file=image_file,\n",
    "        )\n",
    "\n",
    "        image, video = self.preprocess_images(conversation)\n",
    "        text = self.preprocess_text(\n",
    "            conversation,\n",
    "            generation_mode=generation_mode,\n",
    "        )\n",
    "\n",
    "        inputs = self.processor_(\n",
    "            text=[text],\n",
    "            images=image,\n",
    "            videos=video,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:12<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name= \"StanfordAIMI/CheXagent-8b\"\n",
    "\n",
    "model_class = ChexAgent(\n",
    "            model_name_or_path=model_name,\n",
    "            cache_dir = '/netscratch/kadir/xl-vlms/cache/hub',\n",
    "            processor_name=model_name,\n",
    "            local_files_only=False,\n",
    "            trust_remote_code=True,\n",
    "        \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib version for info: 3.9.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "print(\"Matplotlib version for info:\", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10368)\n"
     ]
    }
   ],
   "source": [
    "results_dict = torch.load('/netscratch/kadir/xl-vlms/features/save_hidden_states_for_token_of_interest_chexagent_A_generation_split_train.pth')\n",
    "#print(results_dict['model_predictions'])\n",
    "\n",
    "print(torch.tensor(results_dict['token_of_interest_mask']).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xl_vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
